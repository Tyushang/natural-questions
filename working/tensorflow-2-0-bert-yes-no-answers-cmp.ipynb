{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir('/home/jupyter/kaggle/working')\n",
    "sys.path.extend(['../input/bert-joint-baseline/'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.2.0-dev20200112'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import json\n",
    "import bert_utils\n",
    "import modeling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tokenization\n",
    "\n",
    "import importlib\n",
    "\n",
    "importlib.reload(bert_utils)\n",
    "\n",
    "tf.__version__\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classes & Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                            \"non-floating point (and non-complex) \"\n",
    "                            \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                             \"`TDense` should be defined. \"\n",
    "                             \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size, last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.kernel, transpose_b=True) + self.bias\n",
    "\n",
    "\n",
    "class DummyObject:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "\n",
    "def mk_model(config):\n",
    "    seq_len = config['max_position_embeddings']\n",
    "    example_id = tf.keras.Input(shape=(1,), dtype=tf.int64, name='example_id')\n",
    "    unique_id = tf.keras.Input(shape=(1,), dtype=tf.int64, name='unique_id')\n",
    "    input_ids = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='segment_ids')\n",
    "    BERT = modeling.BertModel(config=config, name='bert')\n",
    "    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n",
    "                                          input_mask=input_mask,\n",
    "                                          input_type_ids=segment_ids)\n",
    "\n",
    "    logits = TDense(2, name='logits')(sequence_output)\n",
    "    start_logits, end_logits = tf.split(logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_squeeze')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_squeeze')\n",
    "\n",
    "    ans_type = TDense(5, name='ans_type')(pooled_output)\n",
    "    return tf.keras.Model([input_ for input_ in [example_id, unique_id, input_ids, input_mask, segment_ids]\n",
    "                           if input_ is not None],\n",
    "                          [example_id, unique_id, start_logits, end_logits, ans_type],\n",
    "                          name='bert-baseline')\n",
    "\n",
    "\n",
    "def url_exists(url):\n",
    "    \"\"\"test local or gs file exists or not.\"\"\"\n",
    "    from urllib import parse\n",
    "    res = parse.urlparse(url)\n",
    "    print(res)\n",
    "    if res.scheme == 'gs':\n",
    "        # blob_name has no '/' prefix\n",
    "        bucket_name, blob_name = res.netloc, res.path[1:]\n",
    "        from google.cloud import storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name[1:])\n",
    "        return blob.exists()\n",
    "    else:\n",
    "        return os.path.exists(res.path)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# url_exists('gs://tyu-kaggle/input/bert-joint-baseline/bert_config.json')\n",
    "# !gsutil ls -R gs://tyu-kaggle/input/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"vocab_size\": 30522,\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"max_position_embeddings\": 512\n",
      "}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "FLAGS = DummyObject(skip_nested_contexts=True,\n",
    "                    max_position=50,\n",
    "                    max_contexts=48,\n",
    "                    max_query_length=64,\n",
    "                    max_seq_length=512,\n",
    "                    doc_stride=128,\n",
    "                    include_unknowns=-1.0,\n",
    "                    n_best_size=20,\n",
    "                    max_answer_length=30)\n",
    "\n",
    "SEQ_LENGTH = FLAGS.max_seq_length  # config['max_position_embeddings']\n",
    "\n",
    "RUN_ON = 'kaggle' if os.path.exists('/kaggle') else 'gcp'\n",
    "\n",
    "if RUN_ON == 'gcp':\n",
    "    INPUT_PATH = 'gs://tyu-kaggle/input/'\n",
    "else:\n",
    "    INPUT_PATH = '../input/'\n",
    "CPKT_PATH = os.path.join(INPUT_PATH, 'bert-joint-baseline/model_cpkt-1')\n",
    "VOCAB_PATH = os.path.join(INPUT_PATH, 'bert-joint-baseline/vocab-nq.txt')\n",
    "NQ_TEST_TFRECORD_PATH = os.path.join(INPUT_PATH, 'bert-joint-baseline/nq-test.tfrecords')\n",
    "\n",
    "NQ_TEST_JSONL_PATH = '../input/tensorflow2-question-answering/simplified-nq-test.jsonl'\n",
    "NQ_TRAIN_JSONL_PATH = '../input/tensorflow2-question-answering/simplified-nq-train.jsonl'\n",
    "\n",
    "TEST_DS_TYPE = 'public' if os.path.getsize(NQ_TEST_JSONL_PATH) < 20000000 else 'private'\n",
    "\n",
    "FEATURE_DESCRIPTION = {\n",
    "    \"example_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "}\n",
    "\n",
    "with open('../input/bert-joint-baseline/bert_config.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "print(json.dumps(config, indent=4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Running on TPU  ['192.168.21.2:8470']\n",
      "INFO:tensorflow:Initializing the TPU system: tyu\n",
      "INFO:tensorflow:Clearing out eager caches\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "REPLICAS:  8\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method BertModel.call of <modeling.BertModel object at 0x7f3e20117588>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp_pkv6ttj.py, line 35)\n",
      "WARNING: AutoGraph could not transform <bound method BertModel.call of <modeling.BertModel object at 0x7f3e20117588>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp_pkv6ttj.py, line 35)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function get_shape_list at 0x7f3e248547b8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpm_wqzh1z.py, line 33)\n",
      "WARNING: AutoGraph could not transform <function get_shape_list at 0x7f3e248547b8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpm_wqzh1z.py, line 33)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method EmbeddingPostprocessor.call of <modeling.EmbeddingPostprocessor object at 0x7f3e241dc8d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmppq4yyofj.py, line 28)\n",
      "WARNING: AutoGraph could not transform <bound method EmbeddingPostprocessor.call of <modeling.EmbeddingPostprocessor object at 0x7f3e241dc8d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmppq4yyofj.py, line 28)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Transformer.call of <modeling.Transformer object at 0x7f3e241dca90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp4gj4hf3a.py, line 47)\n",
      "WARNING: AutoGraph could not transform <bound method Transformer.call of <modeling.Transformer object at 0x7f3e241dca90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp4gj4hf3a.py, line 47)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TransformerBlock.call of <modeling.TransformerBlock object at 0x7f3e2007ee10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp_hlx5qqf.py, line 25)\n",
      "WARNING: AutoGraph could not transform <bound method TransformerBlock.call of <modeling.TransformerBlock object at 0x7f3e2007ee10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp_hlx5qqf.py, line 25)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <modeling.Attention object at 0x7f3e200a8240>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpw3neai_k.py, line 26)\n",
      "WARNING: AutoGraph could not transform <bound method Attention.call of <modeling.Attention object at 0x7f3e200a8240>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpw3neai_k.py, line 26)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Dense3D.call of <modeling.Dense3D object at 0x7f3e26737358>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpv6vbsknc.py, line 31)\n",
      "WARNING: AutoGraph could not transform <bound method Dense3D.call of <modeling.Dense3D object at 0x7f3e26737358>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpv6vbsknc.py, line 31)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Dense2DProjection.call of <modeling.Dense2DProjection object at 0x7f3e200aef28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp2g4apiua.py, line 30)\n",
      "WARNING: AutoGraph could not transform <bound method Dense2DProjection.call of <modeling.Dense2DProjection object at 0x7f3e200aef28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp2g4apiua.py, line 30)\n",
      "Model: \"bert-baseline\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModel)                ((None, 1024), (None 335141888   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "logits (TDense)                 (None, 512, 2)       2050        bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 512, 1), (No 0           logits[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "example_id (InputLayer)         [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "unique_id (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_squeeze (Tens [(None, 512)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_squeeze (Tensor [(None, 512)]        0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "ans_type (TDense)               (None, 5)            5125        bert[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 335,149,063\n",
      "Trainable params: 335,149,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<__main__.TDense object at 0x7f3e200b9048> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f3e200c74a8>).\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tyu\n",
      "INFO:tensorflow:Clearing out eager caches\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method BertModel.call of <modeling.BertModel object at 0x7f3e20117588>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp_pkv6ttj.py, line 35)\n",
      "WARNING:tensorflow:AutoGraph could not transform <function get_shape_list at 0x7f3e248547b8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpm_wqzh1z.py, line 33)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method EmbeddingPostprocessor.call of <modeling.EmbeddingPostprocessor object at 0x7f3e241dc8d0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmppq4yyofj.py, line 28)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Transformer.call of <modeling.Transformer object at 0x7f3e241dca90>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp4gj4hf3a.py, line 47)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TransformerBlock.call of <modeling.TransformerBlock object at 0x7f3e2007ee10>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp_hlx5qqf.py, line 25)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Attention.call of <modeling.Attention object at 0x7f3e200a8240>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpw3neai_k.py, line 26)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Dense3D.call of <modeling.Dense3D object at 0x7f3e26737358>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpv6vbsknc.py, line 31)\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Dense2DProjection.call of <modeling.Dense2DProjection object at 0x7f3e200aef28>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmp2g4apiua.py, line 30)\n",
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n",
      "\n",
      "Two checkpoint references resolved to different objects (<__main__.TDense object at 0x7f3e200b9048> and <tensorflow.python.keras.engine.base_layer.TensorFlowOpLayer object at 0x7f3e200c74a8>).\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', TPU.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
    "    BATCH_SIZE = 32\n",
    "    # drop_remainder must be True if running on TPU, maybe a bug\n",
    "    # so we pad some examples.\n",
    "    nq_test_jsonl_path2 = NQ_TEST_JSONL_PATH + '.pad'\n",
    "    !cp $NQ_TEST_JSONL_PATH $nq_test_jsonl_path2\n",
    "    !tail -n 3 $NQ_TEST_JSONL_PATH >> $nq_test_jsonl_path2\n",
    "    NQ_TEST_JSONL_PATH = nq_test_jsonl_path2\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "with strategy.scope():\n",
    "    model = mk_model(config)\n",
    "    model.summary()\n",
    "    cpkt = tf.train.Checkpoint(model=model)\n",
    "    cpkt.restore(CPKT_PATH).assert_consumed()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# small_config = config.copy()\n",
    "# small_config['vocab_size']=16\n",
    "# small_config['hidden_size']=64\n",
    "# small_config['max_position_embeddings'] = 32\n",
    "# small_config['num_hidden_layers'] = 4\n",
    "# small_config['num_attention_heads'] = 4\n",
    "# small_config['intermediate_size'] = 256\n",
    "# small_config\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Reading: ../input/tensorflow2-question-answering/simplified-nq-test.jsonl.pad\n",
      "\n",
      "number of test examples: 9090, written to file: 9090\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63da87c03f1b48fbb3bf7eca18de1500",
       "version_minor": 0,
       "version_major": 2
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# if not url_exists(NQ_TEST_JSONL_PATH):\n",
    "if True:\n",
    "    # tf2baseline.FLAGS.max_seq_length = 512\n",
    "    eval_writer = bert_utils.FeatureWriter(filename=NQ_TEST_TFRECORD_PATH,\n",
    "                                           is_training=False)\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_PATH,\n",
    "                                           do_lower_case=True)\n",
    "    features = []\n",
    "    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n",
    "                                                  is_training=False,\n",
    "                                                  output_fn=eval_writer.process_feature,\n",
    "                                                  collect_stat=False)\n",
    "    n_examples = 0\n",
    "    tqdm_notebook = tqdm.tqdm_notebook  # if not on_kaggle_server else None\n",
    "    for examples in bert_utils.nq_examples_iter(input_file=NQ_TEST_JSONL_PATH,\n",
    "                                                is_training=False,\n",
    "                                                tqdm=tqdm_notebook):\n",
    "        for example in examples:\n",
    "            n_examples += convert(example)\n",
    "    eval_writer.close()\n",
    "    print('number of test examples: %d, written to file: %d' % (n_examples, eval_writer.num_features))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def _decode_record(record, feature_description=None):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    feature_description = feature_description or FEATURE_DESCRIPTION\n",
    "    example = tf.io.parse_single_example(serialized=record, features=feature_description)\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for key in [k for k in example.keys() if k not in ['example_id', 'unique_id']]:\n",
    "        example[key] = tf.cast(example[key], dtype=tf.int32)\n",
    "\n",
    "    return example\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _decode_record at 0x7f3dbc3f8a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpcgb_oj3h.py, line 18)\n",
      "WARNING: AutoGraph could not transform <function _decode_record at 0x7f3dbc3f8a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpcgb_oj3h.py, line 18)\n",
      "    284/Unknown - 41s 144ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function _decode_record at 0x7f3dbc3f8a60> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: can't assign to () (tmpcgb_oj3h.py, line 18)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "raw_ds = tf.data.TFRecordDataset(NQ_TEST_TFRECORD_PATH)\n",
    "decoded_ds = raw_ds.map(_decode_record)\n",
    "batched_ds = decoded_ds.batch(batch_size=BATCH_SIZE, drop_remainder=(TPU is not None))\n",
    "\n",
    "result = model.predict(batched_ds, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "np.savez_compressed('bert-joint-baseline-output.npz',\n",
    "                    **dict(zip(['uniqe_id', 'start_logits', 'end_logits', 'answer_type_logits'],\n",
    "                               result)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "Span = collections.namedtuple(\"Span\", [\"start_token_idx\", \"end_token_idx\"])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "class ScoreSummary(object):\n",
    "    def __init__(self):\n",
    "        self.predicted_label = None\n",
    "        self.short_span_score = None\n",
    "        self.cls_token_score = None\n",
    "        self.answer_type_logits = None\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class EvalExample(object):\n",
    "    \"\"\"Eval data available for a single example.\"\"\"\n",
    "\n",
    "    def __init__(self, example_id, candidates):\n",
    "        self.example_id = example_id\n",
    "        self.candidates = candidates\n",
    "        self.results = {}\n",
    "        self.features = {}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def get_best_indexes(logits, n_best_size):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(\n",
    "        enumerate(logits[1:], 1), key=lambda x: x[1], reverse=True)\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes\n",
    "\n",
    "\n",
    "def top_k_indices(logits, n_best_size, token_map):\n",
    "    indices = np.argsort(logits[1:]) + 1\n",
    "    indices = indices[token_map[indices] != -1]\n",
    "    return indices[-n_best_size:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1- Understanding the code\n",
    "#### For a better understanding, I will briefly explain here.\n",
    "#### In the item \"answer_type\", in the last lines of this block, it is responsible for storing the identified response type, which, according to [github project repository](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py) can be:\n",
    "1.\n",
    "UNKNOWN = 0\n",
    "2.\n",
    "YES = 1\n",
    "3.\n",
    "NO = 2\n",
    "4.\n",
    "SHORT = 3\n",
    "5.\n",
    "LONG = 4\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def compute_predictions(example):\n",
    "    \"\"\"Converts an example into an NQEval object for evaluation.\"\"\"\n",
    "    predictions = []\n",
    "    n_best_size = FLAGS.n_best_size\n",
    "    max_answer_length = FLAGS.max_answer_length\n",
    "    i = 0\n",
    "    for unique_id, result in example.results.items():\n",
    "        if unique_id not in example.features:\n",
    "            raise ValueError(\"No feature found with unique_id:\", unique_id)\n",
    "        token_map = np.array(example.features[unique_id][\"token_map\"])  # .int64_list.value\n",
    "        start_indexes = top_k_indices(result.start_logits, n_best_size, token_map)\n",
    "        if len(start_indexes) == 0:\n",
    "            continue\n",
    "        end_indexes = top_k_indices(result.end_logits, n_best_size, token_map)\n",
    "        if len(end_indexes) == 0:\n",
    "            continue\n",
    "        indexes = np.array(list(np.broadcast(start_indexes[None], end_indexes[:, None])))\n",
    "        indexes = indexes[(indexes[:, 0] < indexes[:, 1]) * (indexes[:, 1] - indexes[:, 0] < max_answer_length)]\n",
    "        for start_index, end_index in indexes:\n",
    "            summary = ScoreSummary()\n",
    "            summary.short_span_score = (\n",
    "                    result.start_logits[start_index] +\n",
    "                    result.end_logits[end_index])\n",
    "            summary.cls_token_score = (\n",
    "                    result.start_logits[0] + result.end_logits[0])\n",
    "            summary.answer_type_logits = result.answer_type_logits - result.answer_type_logits.mean()\n",
    "            start_span = token_map[start_index]\n",
    "            end_span = token_map[end_index] + 1\n",
    "\n",
    "            # Span logits minus the cls logits seems to be close to the best.\n",
    "            score = summary.short_span_score - summary.cls_token_score\n",
    "            predictions.append((score, i, summary, start_span, end_span))\n",
    "            i += 1  # to break ties\n",
    "\n",
    "    # Default empty prediction.\n",
    "    score = -10000.0\n",
    "    short_span = Span(-1, -1)\n",
    "    long_span = Span(-1, -1)\n",
    "    summary = ScoreSummary()\n",
    "\n",
    "    if predictions:\n",
    "        score, _, summary, start_span, end_span = sorted(predictions, reverse=True)[0]\n",
    "        short_span = Span(start_span, end_span)\n",
    "        for c in example.candidates:\n",
    "            start = short_span.start_token_idx\n",
    "            end = short_span.end_token_idx\n",
    "            ## print(c['top_level'],c['start_token'],s_short_span,c['end_token'],end)\n",
    "            if c[\"top_level\"] and c[\"start_token\"] <= start and c[\"end_token\"] >= end:\n",
    "                long_span = Span(c[\"start_token\"], c[\"end_token\"])\n",
    "                break\n",
    "    summary.predicted_label = {\n",
    "        \"example_id\": int(example.example_id),\n",
    "        \"long_answer\": {\n",
    "            \"start_token\": int(long_span.start_token_idx),\n",
    "            \"end_token\": int(long_span.end_token_idx),\n",
    "            \"start_byte\": -1,\n",
    "            \"end_byte\": -1\n",
    "        },\n",
    "        \"long_answer_score\": float(score),\n",
    "        \"short_answers\": [{\n",
    "            \"start_token\": int(short_span.start_token_idx),\n",
    "            \"end_token\": int(short_span.end_token_idx),\n",
    "            \"start_byte\": -1,\n",
    "            \"end_byte\": -1\n",
    "        }],\n",
    "        \"short_answer_score\": float(score),\n",
    "        \"yes_no_answer\": \"NONE\",\n",
    "        \"answer_type_logits\": summary.answer_type_logits.tolist(),\n",
    "        # here:\n",
    "        \"answer_type\": int(np.argmax(summary.answer_type_logits))\n",
    "    }\n",
    "    \n",
    "    return summary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def compute_pred_dict(candidates_dict, dev_features, raw_results, tqdm=None):\n",
    "    \"\"\"Computes official answer key from raw logits.\"\"\"\n",
    "    raw_results_by_id = [(int(res.unique_id), 1, res) for res in raw_results]\n",
    "    examples_by_id = [(int(k), 0, v) for k, v in candidates_dict.items()]\n",
    "    features_by_id = [(int(d['unique_id']), 2, d) for d in dev_features]\n",
    "    # Join examples with features and raw results.\n",
    "    examples = []\n",
    "    print('merging examples...')\n",
    "    merged = sorted(examples_by_id + raw_results_by_id + features_by_id)\n",
    "    print('done.')\n",
    "    for idx, type_, datum in merged:\n",
    "        if type_ == 0:  # isinstance(datum, list):\n",
    "            examples.append(EvalExample(idx, datum))\n",
    "        elif type_ == 2:  # \"token_map\" in datum:\n",
    "            examples[-1].features[idx] = datum\n",
    "        else:\n",
    "            examples[-1].results[idx] = datum\n",
    "    # Construct prediction objects.\n",
    "    print('Computing predictions...')\n",
    "    nq_pred_dict = {}\n",
    "    # summary_dict = {}\n",
    "    if tqdm is not None:\n",
    "        examples = tqdm(examples)\n",
    "    for e in examples:\n",
    "        summary = compute_predictions(e)\n",
    "        # summary_dict[e.example_id] = summary\n",
    "        nq_pred_dict[e.example_id] = summary.predicted_label\n",
    "\n",
    "    return nq_pred_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def read_candidates_from_one_split(input_path):\n",
    "    \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
    "    candidates_dict = {}\n",
    "    print(\"Reading examples from: %s\" % input_path)\n",
    "    if input_path.endswith(\".gz\"):\n",
    "        with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "    else:\n",
    "        with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "                # candidates_dict['question'] = e['question_text']\n",
    "    return candidates_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def read_candidates(input_pattern):\n",
    "    \"\"\"Read candidates with real multiple processes.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_pattern)\n",
    "    final_dict = {}\n",
    "    for input_path in input_paths:\n",
    "        final_dict.update(read_candidates_from_one_split(input_path))\n",
    "    return final_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "result_df len before: 9088\n",
      "result_df len after : 9070\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "result_df = pd.DataFrame({\n",
    "    \"example_id\": result[0].squeeze().tolist(),\n",
    "    \"unique_id\": result[1].squeeze().tolist(),\n",
    "    \"start_logits\": result[2].tolist(),\n",
    "    \"end_logits\": result[3].tolist(),\n",
    "    \"answer_type_logits\": result[4].tolist()\n",
    "}).set_index(['example_id', 'unique_id'])\n",
    "# we pad some instances when using TPU, deduplicate it here.\n",
    "if TPU is not None:\n",
    "    print('result_df len before: ' + str(len(result_df)))\n",
    "    result_df = result_df[~result_df.index.duplicated()]\n",
    "    print('result_df len after : ' + str(len(result_df)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# candidates_dict['8644948107288181312']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Going to candidates file\n",
      "Reading examples from: ../input/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
      "setting up eval features\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "all_results = [bert_utils.RawResult(*x) for x in zip(*result[1:])]\n",
    "print(\"Going to candidates file\")\n",
    "candidates_dict = read_candidates('../input/tensorflow2-question-answering/simplified-nq-test.jsonl')\n",
    "candidates_df = pd.DataFrame.from_records(list(candidates_dict.items()),\n",
    "                                          columns=['example_id', 'candidates'],\n",
    "                                          index='example_id')\n",
    "candidates_df.index = candidates_df.index.map(lambda x: int(x))\n",
    "print(\"setting up eval features\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "token_map_ds = raw_ds.map(lambda x: tf.io.parse_single_example(\n",
    "    serialized=x,\n",
    "    features={\n",
    "        \"example_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        # token_map: token to origin map.\n",
    "        \"token_map\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64)\n",
    "    }\n",
    "))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "compute_pred_dict\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "eval_features = list(token_map_ds)\n",
    "print(\"compute_pred_dict\")\n",
    "tqdm_notebook = tqdm.tqdm_notebook"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "token_map_df len before: 9090\n",
      "token_map_df len before: 9070\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "token_map_df = pd.DataFrame.from_records(list(token_map_ds)).applymap(\n",
    "    lambda x: x.numpy()\n",
    ").set_index(['example_id', 'unique_id'])\n",
    "\n",
    "# we pad some instances when using TPU, deduplicate it here.\n",
    "if TPU is not None:\n",
    "    print('token_map_df len before: ' + str(len(token_map_df)))\n",
    "    token_map_df = token_map_df[~token_map_df.index.duplicated()]\n",
    "    print('token_map_df len before: ' + str(len(token_map_df)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# token_map_df.head(60)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "joined = result_df.join(token_map_df, on=['example_id', 'unique_id']) \\\n",
    "    .join(candidates_df, on='example_id')\n",
    "\n",
    "pred_df = pd.DataFrame(index=candidates_df.index,\n",
    "                       columns=['score', 'short_span_start', 'short_span_end',\n",
    "                                'long_span_start', 'long_span_end', 'answer_type'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "gg = joined.groupby('example_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/usr/local/lib/python3.5/dist-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for example_id, example_df in gg:\n",
    "    # example_df: each row got a unique id(unique_id), all rows have a some example_id.\n",
    "    # columns = ['answer_type_logits', 'end_logits', 'start_logits', 'token_map', 'candidates']\n",
    "    example_df.reset_index(level='example_id', drop=True, inplace=True)\n",
    "    for u_id, res in example_df.iterrows():\n",
    "        msk_invalid_token = np.array(res['token_map']) == -1\n",
    "        # filter logits corresponding to context token and rank top-k.\n",
    "        s_logits = pd.Series(res['start_logits'])\n",
    "        s_msk_not_top_k = s_logits.mask(msk_invalid_token)\\\n",
    "                              .rank(method='min', ascending=False) > FLAGS.n_best_size\n",
    "        s_indexes = np.ma.masked_array(np.arange(s_logits.size),\n",
    "                                       mask=s_msk_not_top_k | msk_invalid_token)\n",
    "        e_logits = pd.Series(res['end_logits'])\n",
    "        e_msk_not_top_k = e_logits.mask(msk_invalid_token)\\\n",
    "                              .rank(method='min', ascending=False) > FLAGS.n_best_size\n",
    "        e_indexes = np.ma.masked_array(np.arange(e_logits.size),\n",
    "                                       mask=e_msk_not_top_k | msk_invalid_token)\n",
    "        # s_e_msk has shape: [512, 512], end index should greater than start index, otherwise, mask it.\n",
    "        s_e_msk = e_indexes[np.newaxis, :] <= s_indexes[:, np.newaxis]\n",
    "        # answer length should litter than max_answer_length, otherwise, mask it.\n",
    "        s_e_msk |= (e_indexes[np.newaxis, :] - s_indexes[:, np.newaxis] >= FLAGS.max_answer_length)\n",
    "        # full mask.\n",
    "        s_e_msk = s_e_msk.filled(True)\n",
    "        \n",
    "        if s_e_msk.all():  # if all start-end combinations has been masked.\n",
    "            example_df.loc[u_id, 'score'] = np.NAN\n",
    "            example_df.loc[u_id, 's_short_span'] = np.NAN\n",
    "            example_df.loc[u_id, 'e_short_span'] = np.NAN\n",
    "        else:\n",
    "            # broadcast to shape: [512, 512], and set mask=s_e_msk\n",
    "            s_logits_bc = np.ma.array(\n",
    "                np.broadcast_to(s_logits[:, np.newaxis], shape=[s_logits.size, e_logits.size]),\n",
    "                mask=s_e_msk)\n",
    "            e_logits_bc = np.ma.array(\n",
    "                np.broadcast_to(e_logits[np.newaxis, :], shape=[s_logits.size, e_logits.size]),\n",
    "                mask=s_e_msk)\n",
    "            short_span_score = s_logits_bc + e_logits_bc\n",
    "            cls_token_score = s_logits[0] + e_logits[0]\n",
    "            score = short_span_score - cls_token_score\n",
    "            s_short_idx, e_short_idx = divmod(score.argmax(), e_logits.size)\n",
    "            \n",
    "            example_df.loc[u_id, 'score'] = score.max()\n",
    "            example_df.loc[u_id, 's_short_span'] = res['token_map'][s_short_idx]\n",
    "            example_df.loc[u_id, 'e_short_span'] = res['token_map'][e_short_idx] + 1 # end span should be exclusive\n",
    "        answer_type_logits = pd.Series(res['answer_type_logits'], \n",
    "                                       index=['UNKNOWN', 'YES', 'NO', 'SHORT', 'LONG'])\n",
    "        example_df.loc[u_id, 'answer_type'] = answer_type_logits.idxmax()\n",
    "        # break\n",
    "    best_u_id = example_df['score'].idxmax()\n",
    "    if best_u_id is not np.NAN:  # if all instances got no score\n",
    "        short_span_start, short_span_end = example_df.loc[best_u_id, ['s_short_span', 'e_short_span']]\n",
    "        pred_df.loc[example_id, 'score'] = example_df.loc[best_u_id, 'score']\n",
    "        pred_df.loc[example_id, 'short_span_start'] = short_span_start\n",
    "        pred_df.loc[example_id, 'short_span_end'] = short_span_end\n",
    "        # # search for long answer span.\n",
    "        for cand in example_df.iloc[0]['candidates']:\n",
    "            if cand['top_level'] and cand['start_token'] <= short_span_start and short_span_end <= cand['end_token']:\n",
    "                pred_df.loc[example_id, 'long_span_start'] = cand['start_token']\n",
    "                pred_df.loc[example_id, 'long_span_end'] = cand['end_token']\n",
    "                break\n",
    "        pred_df.loc[example_id, 'answer_type'] = example_df.loc[best_u_id, 'answer_type']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "answer_type_logits    [1.0000427961349487, -1.928816318511963, -3.04...\nend_logits            [2.6496543884277344, -9.423047065734863, -9.98...\nstart_logits          [2.9686124324798584, -8.327146530151367, -7.87...\nName: (6801986500551995902, 6801986500551995902), dtype: object"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 30
    }
   ],
   "source": [
    "result_df.loc[(6801986500551995902, 6801986500551995902)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "merging examples...\n",
      "done.\n",
      "Computing predictions...\n",
      "\n",
      "writing json\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:23: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=346.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc697d68e77f4b82808a546ca88ba7d2",
       "version_minor": 0,
       "version_major": 2
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nq_pred_dict = compute_pred_dict(candidates_dict, eval_features[:9070], all_results[:9070], tqdm=tqdm_notebook)\n",
    "predictions_json = {\"predictions\": list(nq_pred_dict.values())}\n",
    "print(\"writing json\")\n",
    "with tf.io.gfile.GFile('predictions.json', \"w\") as f:\n",
    "    json.dump(predictions_json, f, indent=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "(6801986500551995902, {'yes_no_answer': 'NONE', 'answer_type': 4, 'long_answer': {'start_byte': -1, 'start_token': 377, 'end_byte': -1, 'end_token': 415}, 'long_answer_score': -0.29973649978637695, 'answer_type_logits': [1.2898046970367432, -1.6390544176101685, -2.754089117050171, 0.5325103402137756, 2.5708281993865967], 'short_answer_score': -0.29973649978637695, 'short_answers': [{'start_byte': -1, 'start_token': 411, 'end_byte': -1, 'end_token': 414}], 'example_id': 6801986500551995902})\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for x in nq_pred_dict.items():\n",
    "    print(x)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2- Main Change\n",
    "#### Here is the small, but main change: we created an if to check the predicted response type and thus filter / identify the responses that are passed to the submission file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering the Answers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def create_short_answer(entry):\n",
    "    answer = []\n",
    "    if entry['answer_type'] == 0:\n",
    "        return \"\"\n",
    "    elif entry['answer_type'] == 1:\n",
    "        return 'YES'\n",
    "    elif entry['answer_type'] == 2:\n",
    "        return 'NO'\n",
    "    elif entry[\"short_answer_score\"] < 1.5:\n",
    "        return \"\"\n",
    "    else:\n",
    "        for short_answer in entry[\"short_answers\"]:\n",
    "            if short_answer[\"start_token\"] > -1:\n",
    "                answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n",
    "        return \" \".join(answer)\n",
    "\n",
    "\n",
    "def get_short_span(pred_row: pd.Series):\n",
    "    # score(best short answer) is np.NAN means: there's no short/long answers.\n",
    "    if pred_row['score'] is np.NAN:\n",
    "        return ''\n",
    "    # answer_type can not be np.NAN if score is not np.NAN.\n",
    "    if pred_row['answer_type'] == 'UNKNOWN':\n",
    "        return ''\n",
    "    if pred_row['answer_type'] in ['YES', 'NO']:\n",
    "        return pred_row['answer_type']\n",
    "    if pred_row['answer_type'] in ['SHORT', 'LONG']:\n",
    "        if pred_row['score'] < 1.5:\n",
    "            return ''\n",
    "        else:\n",
    "            return '%d:%d' % (pred_row['short_span_start'], pred_row['short_span_end'])\n",
    "\n",
    "\n",
    "def create_long_answer(entry):\n",
    "    answer = []\n",
    "    if entry['answer_type'] == 0:\n",
    "        return ''\n",
    "    elif entry[\"long_answer_score\"] < 1.5:\n",
    "        return \"\"\n",
    "    elif entry[\"long_answer\"][\"start_token\"] > -1:\n",
    "        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n",
    "        return \" \".join(answer)\n",
    "\n",
    "\n",
    "def get_long_span(pred_row: pd.Series):\n",
    "    # score(best short answer) is np.NAN means: there's no short/long answers.\n",
    "    if pred_row['score'] is np.NAN:\n",
    "        return ''\n",
    "    # answer_type can not be np.NAN if score is not np.NAN.\n",
    "    if pred_row['answer_type'] == 'UNKNOWN':\n",
    "        return ''\n",
    "    if pred_row['answer_type'] in ['YES', 'NO', 'SHORT', 'LONG']:\n",
    "        if pred_row['score'] < 1.5 or pred_row['long_span_start'] is np.NAN:\n",
    "            return ''\n",
    "        else:\n",
    "            return '%d:%d' % (pred_row['long_span_start'], pred_row['long_span_end'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a DataFrame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "prediction_df = pred_df.copy()\n",
    "prediction_df['long_span'] = pred_df.apply(get_long_span, axis='columns')\n",
    "prediction_df['short_span'] = pred_df.apply(get_short_span, axis='columns')\n",
    "prediction_df.index = prediction_df.index.map(lambda x: str(x))\n",
    "\n",
    "test_answers_df = pd.read_json(\"../working/predictions.json\")\n",
    "for var_name in ['long_answer_score', 'short_answer_score', 'answer_type']:\n",
    "    test_answers_df[var_name] = test_answers_df['predictions'].apply(lambda q: q[var_name])\n",
    "test_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\n",
    "test_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\n",
    "test_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\n",
    "\n",
    "long_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\n",
    "short_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "my = prediction_df.sort_index()\n",
    "you = test_answers_df[['example_id', 'short_answer_score', 'answer_type', 'short_answer', 'long_answer']]\\\n",
    "    .set_index('example_id').sort_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n",
      "OK\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def compare(theirs: pd.Series, mine: pd.Series):\n",
    "    if theirs['short_answer_score'] - mine['score'] > 1e-3:\n",
    "        return False\n",
    "    if ['UNKNOWN', 'YES', 'NO', 'SHORT', 'LONG'][theirs['answer_type']] != mine['answer_type']:\n",
    "        return False\n",
    "    if theirs['short_answer'] != mine['short_span']:\n",
    "        return False\n",
    "    if theirs['long_answer'] != mine['long_span']:\n",
    "        if not (theirs['long_answer'] is None and mine['long_span'] == ''):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "for i in prediction_df.index:\n",
    "    if compare(you.loc[i], my.loc[i]):\n",
    "        print('OK')\n",
    "    else:\n",
    "        print(you.loc[i])\n",
    "        print(my.loc[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating the Submission File"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"../input/tensorflow2-question-answering/sample_submission.csv\")\n",
    "\n",
    "long_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(\n",
    "    lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\n",
    "short_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(\n",
    "    lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n",
    "\n",
    "sample_submission.loc[\n",
    "    sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\n",
    "sample_submission.loc[\n",
    "    sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "sample_submission2 = sample_submission.copy()\n",
    "sample_submission2 = sample_submission2.set_index('example_id')\n",
    "for eid, row in prediction_df.iterrows():\n",
    "    sample_submission2.loc[eid + '_long', 'PredictionString'] = row['long_span']\n",
    "    sample_submission2.loc[eid + '_short', 'PredictionString'] = row['short_span']\n",
    "sample_submission2 = sample_submission2.reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "                           PredictionString\nexample_id                                 \n-1011141123527297803_long           223:277\n-1011141123527297803_short          224:226\n-1028916936938579349_long                  \n-1028916936938579349_short                 \n-1055197305756217938_long           221:335\n...                                     ...\n930196817123445627_short          2583:2586\n934950704129184964_long             496:616\n934950704129184964_short            515:517\n958723574737344087_long            938:2597\n958723574737344087_short          1704:1706\n\n[692 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PredictionString</th>\n    </tr>\n    <tr>\n      <th>example_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1011141123527297803_long</th>\n      <td>223:277</td>\n    </tr>\n    <tr>\n      <th>-1011141123527297803_short</th>\n      <td>224:226</td>\n    </tr>\n    <tr>\n      <th>-1028916936938579349_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1028916936938579349_short</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1055197305756217938_long</th>\n      <td>221:335</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>930196817123445627_short</th>\n      <td>2583:2586</td>\n    </tr>\n    <tr>\n      <th>934950704129184964_long</th>\n      <td>496:616</td>\n    </tr>\n    <tr>\n      <th>934950704129184964_short</th>\n      <td>515:517</td>\n    </tr>\n    <tr>\n      <th>958723574737344087_long</th>\n      <td>938:2597</td>\n    </tr>\n    <tr>\n      <th>958723574737344087_short</th>\n      <td>1704:1706</td>\n    </tr>\n  </tbody>\n</table>\n<p>692 rows  1 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 89
    }
   ],
   "source": [
    "sample_submission2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "# (sample_submission2 == sample_submission).all()\n",
    "# sample_submission = sample_submission.set_index('example_id')\n",
    "# sample_submission2 = sample_submission2.set_index('example_id')\n",
    "jj = sample_submission.join(sample_submission2, on='example_id', rsuffix='_my')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "                         PredictionString PredictionString_my    cmp\nexample_id                                                          \n-697706369125523422_long             None                      False\n4723874854788782295_long             None                      False",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PredictionString</th>\n      <th>PredictionString_my</th>\n      <th>cmp</th>\n    </tr>\n    <tr>\n      <th>example_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-697706369125523422_long</th>\n      <td>None</td>\n      <td></td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4723874854788782295_long</th>\n      <td>None</td>\n      <td></td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 95
    }
   ],
   "source": [
    "# jj['cmp'] = jj['PredictionString'] == jj['PredictionString_my']\n",
    "jj[jj['cmp'] != True]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "                           PredictionString\nexample_id                                 \n-1011141123527297803_long           223:277\n-1011141123527297803_short          224:226\n-1028916936938579349_long                  \n-1028916936938579349_short                 \n-1055197305756217938_long           221:335\n-1055197305756217938_short          222:226\n-1074129516932871805_long         3119:3210\n-1074129516932871805_short        3153:3155\n-1114334749483663139_long                  \n-1114334749483663139_short                 \n-1152268629614456016_long                  \n-1152268629614456016_short                 \n-1219507076732106786_long                  \n-1219507076732106786_short                 \n-1220107454853145579_long                  \n-1220107454853145579_short                 \n-1237358188352001279_long                  \n-1237358188352001279_short                 \n-1316307078555615068_long                  \n-1316307078555615068_short                 ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PredictionString</th>\n    </tr>\n    <tr>\n      <th>example_id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>-1011141123527297803_long</th>\n      <td>223:277</td>\n    </tr>\n    <tr>\n      <th>-1011141123527297803_short</th>\n      <td>224:226</td>\n    </tr>\n    <tr>\n      <th>-1028916936938579349_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1028916936938579349_short</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1055197305756217938_long</th>\n      <td>221:335</td>\n    </tr>\n    <tr>\n      <th>-1055197305756217938_short</th>\n      <td>222:226</td>\n    </tr>\n    <tr>\n      <th>-1074129516932871805_long</th>\n      <td>3119:3210</td>\n    </tr>\n    <tr>\n      <th>-1074129516932871805_short</th>\n      <td>3153:3155</td>\n    </tr>\n    <tr>\n      <th>-1114334749483663139_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1114334749483663139_short</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1152268629614456016_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1152268629614456016_short</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1219507076732106786_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1219507076732106786_short</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1220107454853145579_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1220107454853145579_short</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1237358188352001279_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1237358188352001279_short</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1316307078555615068_long</th>\n      <td></td>\n    </tr>\n    <tr>\n      <th>-1316307078555615068_short</th>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 91
    }
   ],
   "source": [
    "sample_submission.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Yes\n",
    "Answers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "                     example_id PredictionString\n27   -1651666484583736653_short              YES\n113  -3461207570097431362_short              YES\n303  -8260765274544672220_short              YES\n309   -871487000194429353_short              YES\n553   5962215690907729115_short              YES",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>example_id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>27</th>\n      <td>-1651666484583736653_short</td>\n      <td>YES</td>\n    </tr>\n    <tr>\n      <th>113</th>\n      <td>-3461207570097431362_short</td>\n      <td>YES</td>\n    </tr>\n    <tr>\n      <th>303</th>\n      <td>-8260765274544672220_short</td>\n      <td>YES</td>\n    </tr>\n    <tr>\n      <th>309</th>\n      <td>-871487000194429353_short</td>\n      <td>YES</td>\n    </tr>\n    <tr>\n      <th>553</th>\n      <td>5962215690907729115_short</td>\n      <td>YES</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 42
    }
   ],
   "source": [
    "yes_answers = sample_submission[sample_submission['PredictionString'] == 'YES']\n",
    "yes_answers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No\n",
    "Answers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "                   example_id PredictionString\n469  418890410382116795_short               NO",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>example_id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>469</th>\n      <td>418890410382116795_short</td>\n      <td>NO</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 43
    }
   ],
   "source": [
    "no_answers = sample_submission[sample_submission['PredictionString'] == 'NO']\n",
    "no_answers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Balnk\n",
    "Answers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "                    example_id PredictionString\n2    -1028916936938579349_long                 \n3   -1028916936938579349_short                 \n8    -1114334749483663139_long                 \n9   -1114334749483663139_short                 \n10   -1152268629614456016_long                 ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>example_id</th>\n      <th>PredictionString</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>-1028916936938579349_long</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1028916936938579349_short</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>-1114334749483663139_long</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>-1114334749483663139_short</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>-1152268629614456016_long</td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 44
    }
   ],
   "source": [
    "blank_answers = sample_submission[sample_submission['PredictionString'] == '']\n",
    "blank_answers.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "data": {
      "text/plain": "example_id          205\nPredictionString    205\ndtype: int64"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 45
    }
   ],
   "source": [
    "blank_answers.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I am only sharing modifications that I believe may help. I left out Tunning and any significant code changes I made."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We'll be grateful if someone gets a better understanding and can share what really impacts the assessment. No need to share code, just knowledge.\n",
    "### Thank you!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}