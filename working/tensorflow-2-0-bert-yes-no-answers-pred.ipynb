{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "RUN_ON = 'kaggle' if os.path.exists('/kaggle') else 'gcp'\n",
    "\n",
    "if RUN_ON == 'gcp':\n",
    "    os.chdir('/home/jupyter/kaggle/working')\n",
    "    sys.path.extend(['../input/bert-joint-baseline/'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.1.0'"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 2
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import bert_utils\n",
    "import modeling\n",
    "import tokenization\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import importlib\n",
    "\n",
    "importlib.reload(bert_utils)\n",
    "\n",
    "tf.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Classes & Functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                            \"non-floating point (and non-complex) \"\n",
    "                            \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                             \"`TDense` should be defined. \"\n",
    "                             \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size, last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.kernel, transpose_b=True) + self.bias\n",
    "\n",
    "\n",
    "class DummyObject:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "\n",
    "def mk_model(config):\n",
    "    seq_len = config['max_position_embeddings']\n",
    "    unique_id = tf.keras.Input(shape=(1,), dtype=tf.int64, name='unique_id')\n",
    "    input_ids = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='segment_ids')\n",
    "    BERT = modeling.BertModel(config=config, name='bert')\n",
    "    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n",
    "                                          input_mask=input_mask,\n",
    "                                          input_type_ids=segment_ids)\n",
    "\n",
    "    logits = TDense(2, name='logits')(sequence_output)\n",
    "    start_logits, end_logits = tf.split(logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_squeeze')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_squeeze')\n",
    "\n",
    "    ans_type = TDense(5, name='ans_type')(pooled_output)\n",
    "    return tf.keras.Model([input_ for input_ in [unique_id, input_ids, input_mask, segment_ids]\n",
    "                           if input_ is not None],\n",
    "                          [unique_id, start_logits, end_logits, ans_type],\n",
    "                          name='bert-baseline')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def url_exists(url):\n",
    "    \"\"\"test local or gs file exists or not.\"\"\"\n",
    "    from urllib import parse\n",
    "    res = parse.urlparse(url)\n",
    "    if res.scheme == 'gs':\n",
    "        # blob_name has no '/' prefix\n",
    "        bucket_name, blob_name = res.netloc, res.path[1:]\n",
    "        from google.cloud import storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name[1:])\n",
    "        return blob.exists()\n",
    "    else:\n",
    "        return os.path.exists(res.path)\n",
    "\n",
    "\n",
    "def _decode_record(record, feature_description=None):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    feature_description = feature_description or FEATURE_DESCRIPTION\n",
    "    example = tf.io.parse_single_example(serialized=record, features=feature_description)\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for key in [k for k in example.keys() if k not in ['example_id', 'unique_id']]:\n",
    "        example[key] = tf.cast(example[key], dtype=tf.int32)\n",
    "    \n",
    "    example.pop('example_id')\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def read_candidates_from_one_split(input_path):\n",
    "    \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
    "    candidates_dict = {}\n",
    "    print(\"Reading examples from: %s\" % input_path)\n",
    "    if input_path.endswith(\".gz\"):\n",
    "        with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "    else:\n",
    "        with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "                # candidates_dict['question'] = e['question_text']\n",
    "    return candidates_dict\n",
    "\n",
    "\n",
    "def read_candidates(input_pattern):\n",
    "    \"\"\"Read candidates with real multiple processes.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_pattern)\n",
    "    final_dict = {}\n",
    "    for input_path in input_paths:\n",
    "        final_dict.update(read_candidates_from_one_split(input_path))\n",
    "    return final_dict"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "{\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"vocab_size\": 30522,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"max_position_embeddings\": 512\n",
      "}\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "FLAGS = DummyObject(skip_nested_contexts=True,\n",
    "                    max_position=50,\n",
    "                    max_contexts=48,\n",
    "                    max_query_length=64,\n",
    "                    max_seq_length=512,\n",
    "                    doc_stride=128,\n",
    "                    include_unknowns=-1.0,\n",
    "                    n_best_size=20,\n",
    "                    max_answer_length=30)\n",
    "\n",
    "SEQ_LENGTH = FLAGS.max_seq_length  # config['max_position_embeddings']\n",
    "\n",
    "if RUN_ON == 'gcp':\n",
    "    INPUT_PATH = 'gs://tyu-kaggle/input/'\n",
    "else:\n",
    "    INPUT_PATH = '../input/'\n",
    "BERT_CONFIG_PATH = os.path.join('../input', 'bert-joint-baseline/bert_config.json')\n",
    "# CPKT_PATH = os.path.join(INPUT_PATH, 'bert-joint-baseline/model_cpkt-1')\n",
    "CPKT_PATH = 'gs://tyu-kaggle/output/model.ckpt-23187'\n",
    "VOCAB_PATH = os.path.join(INPUT_PATH, 'bert-joint-baseline/vocab-nq.txt')\n",
    "\n",
    "NQ_TEST_JSONL_PATH = '../input/tensorflow2-question-answering/simplified-nq-test.jsonl'\n",
    "NQ_TRAIN_JSONL_PATH = '../input/tensorflow2-question-answering/simplified-nq-train.jsonl'\n",
    "NQ_TEST_TFRECORD_PATH = './nq-test.tfrecords'\n",
    "\n",
    "SAMPLE_SUBMISSION_PATH = '../input/tensorflow2-question-answering/sample_submission.csv'\n",
    "\n",
    "TEST_DS_TYPE = 'public' if os.path.getsize(NQ_TEST_JSONL_PATH) < 20000000 else 'private'\n",
    "\n",
    "FEATURE_DESCRIPTION = {\n",
    "    \"example_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "}\n",
    "ANSWER_TYPE_ORDER = ['UNKNOWN', 'YES', 'NO', 'SHORT', 'LONG']\n",
    "\n",
    "with open(BERT_CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "print(json.dumps(config, indent=4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Running on TPU  ['10.254.212.146:8470']\n",
      "INFO:tensorflow:Initializing the TPU system: tyu\n",
      "INFO:tensorflow:Clearing out eager caches\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "REPLICAS:  8\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: tyu\n",
      "INFO:tensorflow:Clearing out eager caches\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', TPU.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
    "    BATCH_SIZE = 128\n",
    "    # drop_remainder must be True if running on TPU, maybe a bug\n",
    "    # so we pad some examples.\n",
    "    nq_test_jsonl_path2 = NQ_TEST_JSONL_PATH + '.pad'\n",
    "    !cp $NQ_TEST_JSONL_PATH $nq_test_jsonl_path2\n",
    "    !tail -n 3 $NQ_TEST_JSONL_PATH >> $nq_test_jsonl_path2\n",
    "    NQ_TEST_JSONL_PATH = nq_test_jsonl_path2\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "bert/encoder/layer_19/self_attention/query/kernel:0\n",
      "bert/encoder/layer_5/self_attention/query/bias:0\n",
      "bert/encoder/layer_2/self_attention/query/bias:0\n",
      "bert/encoder/layer_1/output/bias:0\n",
      "bert/encoder/layer_22/self_attention_output/bias:0\n",
      "bert/encoder/layer_21/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_0/intermediate/kernel:0\n",
      "bert/encoder/layer_7/self_attention_output/kernel:0\n",
      "bert/encoder/layer_12/output/kernel:0\n",
      "bert/encoder/layer_0/self_attention/value/kernel:0\n",
      "bert/encoder/layer_0/output/kernel:0\n",
      "bert/pooler_transform/kernel:0\n",
      "bert/encoder/layer_12/output/bias:0\n",
      "bert/encoder/layer_9/output_layer_norm/beta:0\n",
      "ans_type/bias:0\n",
      "bert/encoder/layer_2/self_attention_output/bias:0\n",
      "bert/encoder/layer_2/self_attention/value/bias:0\n",
      "bert/encoder/layer_8/output_layer_norm/beta:0\n",
      "bert/encoder/layer_0/self_attention/key/kernel:0\n",
      "bert/encoder/layer_20/self_attention/query/kernel:0\n",
      "bert/encoder/layer_1/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_19/intermediate/bias:0\n",
      "bert/encoder/layer_0/self_attention/key/bias:0\n",
      "bert/encoder/layer_3/self_attention_output/kernel:0\n",
      "bert/encoder/layer_2/self_attention/key/bias:0\n",
      "bert/encoder/layer_14/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_4/output/bias:0\n",
      "bert/encoder/layer_7/output/kernel:0\n",
      "bert/encoder/layer_6/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_8/intermediate/kernel:0\n",
      "bert/encoder/layer_17/intermediate/kernel:0\n",
      "bert/encoder/layer_6/self_attention/value/bias:0\n",
      "bert/encoder/layer_14/output_layer_norm/beta:0\n",
      "bert/encoder/layer_6/self_attention_output/bias:0\n",
      "bert/encoder/layer_4/output_layer_norm/beta:0\n",
      "bert/encoder/layer_2/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_16/self_attention_output/kernel:0\n",
      "bert/encoder/layer_11/self_attention/value/kernel:0\n",
      "bert/encoder/layer_4/self_attention/query/bias:0\n",
      "bert/encoder/layer_2/self_attention/query/kernel:0\n",
      "bert/encoder/layer_5/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_9/self_attention/key/kernel:0\n",
      "bert/encoder/layer_12/intermediate/bias:0\n",
      "bert/encoder/layer_2/output_layer_norm/beta:0\n",
      "bert/encoder/layer_6/output/bias:0\n",
      "bert/encoder/layer_5/output/bias:0\n",
      "bert/encoder/layer_5/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_9/self_attention_output/bias:0\n",
      "bert/encoder/layer_1/self_attention/query/kernel:0\n",
      "bert/encoder/layer_23/self_attention/value/bias:0\n",
      "bert/encoder/layer_9/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_8/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_22/self_attention_output/kernel:0\n",
      "bert/encoder/layer_0/self_attention_output/kernel:0\n",
      "bert/encoder/layer_10/self_attention_output/bias:0\n",
      "bert/encoder/layer_13/self_attention/key/kernel:0\n",
      "bert/encoder/layer_10/self_attention/value/kernel:0\n",
      "bert/encoder/layer_21/self_attention/value/kernel:0\n",
      "bert/encoder/layer_13/self_attention/query/kernel:0\n",
      "bert/encoder/layer_14/intermediate/bias:0\n",
      "bert/encoder/layer_13/self_attention/value/kernel:0\n",
      "bert/encoder/layer_10/self_attention/key/bias:0\n",
      "bert/encoder/layer_7/intermediate/bias:0\n",
      "bert/encoder/layer_23/self_attention_output/kernel:0\n",
      "bert/encoder/layer_3/self_attention/value/bias:0\n",
      "bert/encoder/layer_19/self_attention_output/kernel:0\n",
      "bert/encoder/layer_16/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_18/self_attention/value/kernel:0\n",
      "bert/encoder/layer_23/output/bias:0\n",
      "bert/encoder/layer_21/self_attention/key/kernel:0\n",
      "bert/encoder/layer_7/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_9/self_attention/value/bias:0\n",
      "bert/encoder/layer_21/intermediate/bias:0\n",
      "bert/encoder/layer_23/output_layer_norm/beta:0\n",
      "bert/encoder/layer_5/output_layer_norm/beta:0\n",
      "bert/encoder/layer_19/output_layer_norm/gamma:0\n",
      "bert/embedding_postprocessor/type_embeddings:0\n",
      "bert/encoder/layer_5/self_attention_output/bias:0\n",
      "bert/encoder/layer_6/self_attention/query/kernel:0\n",
      "bert/encoder/layer_6/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_11/self_attention_output/kernel:0\n",
      "bert/encoder/layer_18/self_attention/value/bias:0\n",
      "bert/encoder/layer_20/output/kernel:0\n",
      "bert/encoder/layer_11/output/kernel:0\n",
      "bert/encoder/layer_1/self_attention_output/kernel:0\n",
      "bert/encoder/layer_15/intermediate/kernel:0\n",
      "bert/embedding_postprocessor/layer_norm/gamma:0\n",
      "bert/encoder/layer_3/output/bias:0\n",
      "bert/encoder/layer_6/self_attention/key/bias:0\n",
      "bert/encoder/layer_13/self_attention_output/kernel:0\n",
      "bert/encoder/layer_23/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_10/self_attention/key/kernel:0\n",
      "bert/encoder/layer_22/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_0/self_attention/query/kernel:0\n",
      "bert/encoder/layer_0/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_21/output/kernel:0\n",
      "bert/encoder/layer_8/self_attention_output/kernel:0\n",
      "bert/encoder/layer_13/self_attention/key/bias:0\n",
      "bert/encoder/layer_7/self_attention/key/kernel:0\n",
      "bert/encoder/layer_19/self_attention/value/kernel:0\n",
      "bert/encoder/layer_13/output_layer_norm/beta:0\n",
      "bert/encoder/layer_5/output/kernel:0\n",
      "bert/encoder/layer_1/intermediate/bias:0\n",
      "bert/encoder/layer_6/output/kernel:0\n",
      "bert/encoder/layer_18/output/kernel:0\n",
      "bert/encoder/layer_7/self_attention/value/kernel:0\n",
      "bert/encoder/layer_19/output/kernel:0\n",
      "bert/encoder/layer_20/intermediate/bias:0\n",
      "bert/encoder/layer_4/self_attention/key/kernel:0\n",
      "bert/encoder/layer_8/self_attention/value/kernel:0\n",
      "bert/pooler_transform/bias:0\n",
      "bert/encoder/layer_17/self_attention/key/bias:0\n",
      "bert/encoder/layer_15/self_attention/query/bias:0\n",
      "bert/encoder/layer_17/self_attention/key/kernel:0\n",
      "bert/encoder/layer_10/self_attention/query/kernel:0\n",
      "bert/encoder/layer_9/self_attention_output/kernel:0\n",
      "bert/encoder/layer_18/output_layer_norm/beta:0\n",
      "bert/encoder/layer_20/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_15/output/kernel:0\n",
      "bert/encoder/layer_10/output/kernel:0\n",
      "bert/encoder/layer_21/self_attention/query/bias:0\n",
      "logits/kernel:0\n",
      "bert/encoder/layer_8/output/bias:0\n",
      "bert/encoder/layer_1/self_attention/value/bias:0\n",
      "bert/encoder/layer_11/self_attention_output/bias:0\n",
      "bert/encoder/layer_19/output/bias:0\n",
      "bert/encoder/layer_0/self_attention/query/bias:0\n",
      "bert/encoder/layer_9/intermediate/bias:0\n",
      "bert/encoder/layer_23/intermediate/bias:0\n",
      "bert/encoder/layer_12/self_attention_output/kernel:0\n",
      "bert/encoder/layer_22/intermediate/bias:0\n",
      "bert/encoder/layer_10/intermediate/kernel:0\n",
      "bert/encoder/layer_15/intermediate/bias:0\n",
      "bert/encoder/layer_19/intermediate/kernel:0\n",
      "bert/encoder/layer_12/self_attention/query/kernel:0\n",
      "bert/encoder/layer_4/intermediate/bias:0\n",
      "bert/encoder/layer_18/self_attention_output/bias:0\n",
      "bert/encoder/layer_8/self_attention_output/bias:0\n",
      "bert/encoder/layer_4/self_attention/query/kernel:0\n",
      "bert/encoder/layer_3/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_11/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_11/intermediate/kernel:0\n",
      "bert/encoder/layer_16/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_13/self_attention/value/bias:0\n",
      "bert/encoder/layer_2/self_attention/value/kernel:0\n",
      "bert/encoder/layer_14/output/kernel:0\n",
      "bert/encoder/layer_14/intermediate/kernel:0\n",
      "bert/encoder/layer_10/output/bias:0\n",
      "bert/encoder/layer_21/self_attention_output/kernel:0\n",
      "bert/encoder/layer_4/self_attention/value/bias:0\n",
      "bert/encoder/layer_17/intermediate/bias:0\n",
      "bert/encoder/layer_22/self_attention/query/bias:0\n",
      "bert/encoder/layer_20/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_21/self_attention_output/bias:0\n",
      "bert/encoder/layer_7/output_layer_norm/beta:0\n",
      "bert/encoder/layer_8/self_attention/key/bias:0\n",
      "bert/encoder/layer_17/self_attention/query/bias:0\n",
      "bert/encoder/layer_5/self_attention/key/kernel:0\n",
      "bert/encoder/layer_2/output/bias:0\n",
      "bert/encoder/layer_3/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_11/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_14/self_attention/key/kernel:0\n",
      "bert/encoder/layer_14/self_attention/query/bias:0\n",
      "bert/encoder/layer_11/self_attention/query/kernel:0\n",
      "bert/encoder/layer_0/self_attention/value/bias:0\n",
      "bert/encoder/layer_5/intermediate/kernel:0\n",
      "bert/encoder/layer_20/output/bias:0\n",
      "bert/encoder/layer_4/self_attention/key/bias:0\n",
      "bert/encoder/layer_16/self_attention/query/kernel:0\n",
      "bert/encoder/layer_20/self_attention/query/bias:0\n",
      "bert/encoder/layer_5/intermediate/bias:0\n",
      "bert/encoder/layer_18/self_attention/query/bias:0\n",
      "bert/encoder/layer_19/self_attention/query/bias:0\n",
      "bert/encoder/layer_3/output_layer_norm/gamma:0\n",
      "logits/bias:0\n",
      "bert/encoder/layer_0/intermediate/bias:0\n",
      "bert/encoder/layer_22/self_attention/value/kernel:0\n",
      "bert/encoder/layer_16/intermediate/kernel:0\n",
      "bert/encoder/layer_18/intermediate/bias:0\n",
      "bert/encoder/layer_5/self_attention/query/kernel:0\n",
      "bert/encoder/layer_0/output_layer_norm/beta:0\n",
      "bert/encoder/layer_22/self_attention/key/kernel:0\n",
      "bert/encoder/layer_20/self_attention_output/bias:0\n",
      "bert/encoder/layer_23/intermediate/kernel:0\n",
      "bert/encoder/layer_15/self_attention/query/kernel:0\n",
      "bert/encoder/layer_7/self_attention/query/bias:0\n",
      "bert/encoder/layer_23/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_16/self_attention/value/kernel:0\n",
      "bert/encoder/layer_12/output_layer_norm/beta:0\n",
      "bert/encoder/layer_12/self_attention/query/bias:0\n",
      "bert/encoder/layer_20/self_attention_output/kernel:0\n",
      "bert/encoder/layer_11/intermediate/bias:0\n",
      "bert/encoder/layer_22/output/bias:0\n",
      "bert/encoder/layer_18/intermediate/kernel:0\n",
      "bert/encoder/layer_14/output/bias:0\n",
      "bert/encoder/layer_18/self_attention/key/kernel:0\n",
      "bert/encoder/layer_8/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_13/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_17/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_8/self_attention/query/bias:0\n",
      "bert/encoder/layer_10/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_2/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_16/output/kernel:0\n",
      "bert/encoder/layer_10/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_21/self_attention/query/kernel:0\n",
      "bert/encoder/layer_22/output/kernel:0\n",
      "bert/encoder/layer_0/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_13/self_attention_output/bias:0\n",
      "bert/encoder/layer_1/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_19/self_attention/key/bias:0\n",
      "bert/encoder/layer_15/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_18/self_attention/key/bias:0\n",
      "bert/encoder/layer_4/self_attention_output/kernel:0\n",
      "bert/encoder/layer_0/self_attention_output/bias:0\n",
      "bert/encoder/layer_3/self_attention/value/kernel:0\n",
      "bert/encoder/layer_9/self_attention/value/kernel:0\n",
      "bert/encoder/layer_21/output_layer_norm/beta:0\n",
      "bert/encoder/layer_4/output/kernel:0\n",
      "bert/encoder/layer_6/self_attention_output/kernel:0\n",
      "bert/encoder/layer_3/self_attention/query/kernel:0\n",
      "bert/encoder/layer_13/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_16/self_attention/query/bias:0\n",
      "bert/encoder/layer_23/self_attention/key/bias:0\n",
      "bert/encoder/layer_7/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_21/self_attention/value/bias:0\n",
      "bert/encoder/layer_9/output/kernel:0\n",
      "bert/encoder/layer_8/self_attention/value/bias:0\n",
      "bert/encoder/layer_1/self_attention/key/bias:0\n",
      "bert/encoder/layer_20/self_attention/key/kernel:0\n",
      "bert/encoder/layer_14/self_attention/query/kernel:0\n",
      "bert/encoder/layer_16/output/bias:0\n",
      "bert/encoder/layer_17/output/kernel:0\n",
      "bert/encoder/layer_7/self_attention_output/bias:0\n",
      "bert/encoder/layer_11/self_attention/key/kernel:0\n",
      "bert/encoder/layer_9/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_19/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_3/intermediate/kernel:0\n",
      "bert/encoder/layer_15/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_10/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_2/intermediate/kernel:0\n",
      "bert/encoder/layer_5/self_attention/key/bias:0\n",
      "bert/encoder/layer_17/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_15/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_23/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_22/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_20/output_layer_norm/beta:0\n",
      "bert/encoder/layer_2/intermediate/bias:0\n",
      "bert/encoder/layer_17/output/bias:0\n",
      "bert/encoder/layer_15/output/bias:0\n",
      "bert/encoder/layer_10/self_attention/value/bias:0\n",
      "bert/encoder/layer_15/output_layer_norm/beta:0\n",
      "bert/encoder/layer_7/self_attention/value/bias:0\n",
      "bert/encoder/layer_23/self_attention/value/kernel:0\n",
      "bert/encoder/layer_9/intermediate/kernel:0\n",
      "bert/encoder/layer_0/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_7/output/bias:0\n",
      "bert/encoder/layer_6/self_attention/query/bias:0\n",
      "bert/encoder/layer_4/self_attention_output/bias:0\n",
      "bert/encoder/layer_12/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_10/intermediate/bias:0\n",
      "bert/encoder/layer_8/output/kernel:0\n",
      "bert/encoder/layer_9/self_attention/query/kernel:0\n",
      "bert/encoder/layer_11/self_attention/query/bias:0\n",
      "bert/encoder/layer_7/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_11/output_layer_norm/beta:0\n",
      "bert/encoder/layer_3/self_attention_output/bias:0\n",
      "bert/encoder/layer_8/intermediate/bias:0\n",
      "bert/encoder/layer_22/self_attention/query/kernel:0\n",
      "bert/encoder/layer_12/self_attention_output/bias:0\n",
      "bert/encoder/layer_20/intermediate/kernel:0\n",
      "bert/encoder/layer_0/output/bias:0\n",
      "bert/encoder/layer_22/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_17/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_6/self_attention/value/kernel:0\n",
      "bert/encoder/layer_20/self_attention/value/kernel:0\n",
      "bert/encoder/layer_1/self_attention/query/bias:0\n",
      "bert/encoder/layer_8/self_attention/key/kernel:0\n",
      "bert/encoder/layer_21/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_15/self_attention/value/bias:0\n",
      "bert/encoder/layer_22/intermediate/kernel:0\n",
      "bert/encoder/layer_2/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_12/self_attention/value/kernel:0\n",
      "bert/encoder/layer_9/output/bias:0\n",
      "bert/encoder/layer_13/self_attention_layer_norm/beta:0\n",
      "bert/embedding_postprocessor/position_embeddings:0\n",
      "bert/encoder/layer_12/self_attention/value/bias:0\n",
      "bert/encoder/layer_4/self_attention/value/kernel:0\n",
      "bert/encoder/layer_17/self_attention_output/bias:0\n",
      "bert/encoder/layer_16/output_layer_norm/beta:0\n",
      "bert/encoder/layer_16/self_attention_output/bias:0\n",
      "bert/encoder/layer_1/output/kernel:0\n",
      "bert/encoder/layer_5/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_16/self_attention/key/kernel:0\n",
      "bert/encoder/layer_7/self_attention/query/kernel:0\n",
      "bert/encoder/layer_9/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_1/self_attention/key/kernel:0\n",
      "bert/encoder/layer_17/self_attention/value/kernel:0\n",
      "bert/encoder/layer_20/self_attention/value/bias:0\n",
      "bert/encoder/layer_19/self_attention/key/kernel:0\n",
      "bert/encoder/layer_15/self_attention_output/bias:0\n",
      "bert/encoder/layer_9/self_attention/query/bias:0\n",
      "bert/encoder/layer_10/output_layer_norm/beta:0\n",
      "bert/encoder/layer_1/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_18/output/bias:0\n",
      "bert/encoder/layer_11/self_attention/value/bias:0\n",
      "bert/encoder/layer_18/self_attention/query/kernel:0\n",
      "bert/encoder/layer_11/self_attention/key/bias:0\n",
      "bert/encoder/layer_6/output_layer_norm/beta:0\n",
      "bert/encoder/layer_15/self_attention_output/kernel:0\n",
      "bert/encoder/layer_14/self_attention/value/kernel:0\n",
      "bert/encoder/layer_3/intermediate/bias:0\n",
      "bert/encoder/layer_21/intermediate/kernel:0\n",
      "bert/encoder/layer_4/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_22/self_attention/value/bias:0\n",
      "bert/encoder/layer_19/self_attention_output/bias:0\n",
      "bert/encoder/layer_12/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_1/self_attention_output/bias:0\n",
      "bert/encoder/layer_2/self_attention/key/kernel:0\n",
      "bert/encoder/layer_6/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_14/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_17/self_attention/query/kernel:0\n",
      "bert/encoder/layer_23/self_attention/query/kernel:0\n",
      "bert/encoder/layer_18/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_5/self_attention/value/bias:0\n",
      "bert/encoder/layer_6/intermediate/bias:0\n",
      "bert/encoder/layer_13/output/kernel:0\n",
      "bert/encoder/layer_12/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_13/self_attention/query/bias:0\n",
      "bert/encoder/layer_13/output/bias:0\n",
      "bert/encoder/layer_3/self_attention/key/bias:0\n",
      "bert/encoder/layer_6/self_attention/key/kernel:0\n",
      "bert/encoder/layer_23/self_attention/key/kernel:0\n",
      "bert/encoder/layer_18/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_21/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_19/output_layer_norm/beta:0\n",
      "bert/encoder/layer_14/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_3/self_attention/key/kernel:0\n",
      "bert/encoder/layer_14/self_attention_output/kernel:0\n",
      "bert/encoder/layer_6/intermediate/kernel:0\n",
      "bert/encoder/layer_23/self_attention_output/bias:0\n",
      "bert/encoder/layer_12/self_attention/key/kernel:0\n",
      "bert/encoder/layer_7/self_attention/key/bias:0\n",
      "bert/word_embeddings/embeddings:0\n",
      "bert/embedding_postprocessor/layer_norm/beta:0\n",
      "bert/encoder/layer_3/output_layer_norm/beta:0\n",
      "bert/encoder/layer_8/self_attention/query/kernel:0\n",
      "bert/encoder/layer_17/self_attention/value/bias:0\n",
      "bert/encoder/layer_7/intermediate/kernel:0\n",
      "bert/encoder/layer_17/self_attention_output/kernel:0\n",
      "bert/encoder/layer_16/intermediate/bias:0\n",
      "bert/encoder/layer_3/output/kernel:0\n",
      "bert/encoder/layer_10/self_attention/query/bias:0\n",
      "bert/encoder/layer_2/output/kernel:0\n",
      "bert/encoder/layer_13/intermediate/kernel:0\n",
      "bert/encoder/layer_19/self_attention/value/bias:0\n",
      "bert/encoder/layer_15/self_attention/value/kernel:0\n",
      "bert/encoder/layer_22/output_layer_norm/beta:0\n",
      "bert/encoder/layer_20/self_attention/key/bias:0\n",
      "bert/encoder/layer_17/output_layer_norm/beta:0\n",
      "bert/encoder/layer_21/self_attention/key/bias:0\n",
      "bert/encoder/layer_14/self_attention/value/bias:0\n",
      "bert/encoder/layer_9/self_attention/key/bias:0\n",
      "bert/encoder/layer_18/self_attention_output/kernel:0\n",
      "bert/encoder/layer_22/self_attention/key/bias:0\n",
      "bert/encoder/layer_16/self_attention/key/bias:0\n",
      "bert/encoder/layer_1/output_layer_norm/beta:0\n",
      "bert/encoder/layer_1/self_attention/value/kernel:0\n",
      "bert/encoder/layer_3/self_attention/query/bias:0\n",
      "bert/encoder/layer_14/self_attention_output/bias:0\n",
      "bert/encoder/layer_21/output/bias:0\n",
      "bert/encoder/layer_11/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_16/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_5/self_attention/value/kernel:0\n",
      "bert/encoder/layer_14/self_attention/key/bias:0\n",
      "bert/encoder/layer_12/self_attention/key/bias:0\n",
      "bert/encoder/layer_11/output/bias:0\n",
      "bert/encoder/layer_13/intermediate/bias:0\n",
      "bert/encoder/layer_15/self_attention/key/bias:0\n",
      "bert/encoder/layer_15/self_attention/key/kernel:0\n",
      "bert/encoder/layer_23/self_attention/query/bias:0\n",
      "bert/encoder/layer_5/self_attention_output/kernel:0\n",
      "bert/encoder/layer_10/self_attention_output/kernel:0\n",
      "bert/encoder/layer_12/intermediate/kernel:0\n",
      "bert/encoder/layer_23/output/kernel:0\n",
      "bert/encoder/layer_1/intermediate/kernel:0\n",
      "bert/encoder/layer_16/self_attention/value/bias:0\n",
      "bert/encoder/layer_8/output_layer_norm/gamma:0\n",
      "bert/encoder/layer_19/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_18/output_layer_norm/gamma:0\n",
      "ans_type/kernel:0\n",
      "bert/encoder/layer_2/self_attention_output/kernel:0\n",
      "bert/encoder/layer_4/self_attention_layer_norm/gamma:0\n",
      "bert/encoder/layer_4/self_attention_layer_norm/beta:0\n",
      "bert/encoder/layer_4/intermediate/kernel:0\n",
      "bert/encoder/layer_20/self_attention_layer_norm/gamma:0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "for x in model_params.keys():\n",
    "    print(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['answer_type_output_bias',\n 'answer_type_output_bias/adam_m',\n 'answer_type_output_bias/adam_v',\n 'answer_type_output_weights',\n 'answer_type_output_weights/adam_m',\n 'answer_type_output_weights/adam_v',\n 'bert/embeddings/LayerNorm/beta',\n 'bert/embeddings/LayerNorm/beta/adam_m',\n 'bert/embeddings/LayerNorm/beta/adam_v',\n 'bert/embeddings/LayerNorm/gamma',\n 'bert/embeddings/LayerNorm/gamma/adam_m',\n 'bert/embeddings/LayerNorm/gamma/adam_v',\n 'bert/embeddings/position_embeddings',\n 'bert/embeddings/position_embeddings/adam_m',\n 'bert/embeddings/position_embeddings/adam_v',\n 'bert/embeddings/token_type_embeddings',\n 'bert/embeddings/token_type_embeddings/adam_m',\n 'bert/embeddings/token_type_embeddings/adam_v',\n 'bert/embeddings/word_embeddings',\n 'bert/embeddings/word_embeddings/adam_m',\n 'bert/embeddings/word_embeddings/adam_v',\n 'bert/encoder/layer_0/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_0/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_0/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_0/attention/output/dense/bias',\n 'bert/encoder/layer_0/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_0/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_0/attention/output/dense/kernel',\n 'bert/encoder/layer_0/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_0/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_0/attention/self/key/bias',\n 'bert/encoder/layer_0/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_0/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_0/attention/self/key/kernel',\n 'bert/encoder/layer_0/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_0/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_0/attention/self/query/bias',\n 'bert/encoder/layer_0/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_0/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_0/attention/self/query/kernel',\n 'bert/encoder/layer_0/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_0/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_0/attention/self/value/bias',\n 'bert/encoder/layer_0/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_0/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_0/attention/self/value/kernel',\n 'bert/encoder/layer_0/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_0/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_0/intermediate/dense/bias',\n 'bert/encoder/layer_0/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_0/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_0/intermediate/dense/kernel',\n 'bert/encoder/layer_0/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_0/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_0/output/LayerNorm/beta',\n 'bert/encoder/layer_0/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_0/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_0/output/LayerNorm/gamma',\n 'bert/encoder/layer_0/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_0/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_0/output/dense/bias',\n 'bert/encoder/layer_0/output/dense/bias/adam_m',\n 'bert/encoder/layer_0/output/dense/bias/adam_v',\n 'bert/encoder/layer_0/output/dense/kernel',\n 'bert/encoder/layer_0/output/dense/kernel/adam_m',\n 'bert/encoder/layer_0/output/dense/kernel/adam_v',\n 'bert/encoder/layer_1/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_1/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_1/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_1/attention/output/dense/bias',\n 'bert/encoder/layer_1/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_1/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_1/attention/output/dense/kernel',\n 'bert/encoder/layer_1/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_1/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_1/attention/self/key/bias',\n 'bert/encoder/layer_1/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_1/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_1/attention/self/key/kernel',\n 'bert/encoder/layer_1/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_1/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_1/attention/self/query/bias',\n 'bert/encoder/layer_1/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_1/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_1/attention/self/query/kernel',\n 'bert/encoder/layer_1/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_1/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_1/attention/self/value/bias',\n 'bert/encoder/layer_1/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_1/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_1/attention/self/value/kernel',\n 'bert/encoder/layer_1/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_1/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_1/intermediate/dense/bias',\n 'bert/encoder/layer_1/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_1/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_1/intermediate/dense/kernel',\n 'bert/encoder/layer_1/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_1/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_1/output/LayerNorm/beta',\n 'bert/encoder/layer_1/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_1/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_1/output/LayerNorm/gamma',\n 'bert/encoder/layer_1/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_1/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_1/output/dense/bias',\n 'bert/encoder/layer_1/output/dense/bias/adam_m',\n 'bert/encoder/layer_1/output/dense/bias/adam_v',\n 'bert/encoder/layer_1/output/dense/kernel',\n 'bert/encoder/layer_1/output/dense/kernel/adam_m',\n 'bert/encoder/layer_1/output/dense/kernel/adam_v',\n 'bert/encoder/layer_10/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_10/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_10/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_10/attention/output/dense/bias',\n 'bert/encoder/layer_10/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_10/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_10/attention/output/dense/kernel',\n 'bert/encoder/layer_10/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_10/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_10/attention/self/key/bias',\n 'bert/encoder/layer_10/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_10/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_10/attention/self/key/kernel',\n 'bert/encoder/layer_10/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_10/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_10/attention/self/query/bias',\n 'bert/encoder/layer_10/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_10/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_10/attention/self/query/kernel',\n 'bert/encoder/layer_10/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_10/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_10/attention/self/value/bias',\n 'bert/encoder/layer_10/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_10/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_10/attention/self/value/kernel',\n 'bert/encoder/layer_10/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_10/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_10/intermediate/dense/bias',\n 'bert/encoder/layer_10/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_10/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_10/intermediate/dense/kernel',\n 'bert/encoder/layer_10/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_10/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_10/output/LayerNorm/beta',\n 'bert/encoder/layer_10/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_10/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_10/output/LayerNorm/gamma',\n 'bert/encoder/layer_10/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_10/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_10/output/dense/bias',\n 'bert/encoder/layer_10/output/dense/bias/adam_m',\n 'bert/encoder/layer_10/output/dense/bias/adam_v',\n 'bert/encoder/layer_10/output/dense/kernel',\n 'bert/encoder/layer_10/output/dense/kernel/adam_m',\n 'bert/encoder/layer_10/output/dense/kernel/adam_v',\n 'bert/encoder/layer_11/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_11/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_11/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_11/attention/output/dense/bias',\n 'bert/encoder/layer_11/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_11/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_11/attention/output/dense/kernel',\n 'bert/encoder/layer_11/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_11/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_11/attention/self/key/bias',\n 'bert/encoder/layer_11/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_11/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_11/attention/self/key/kernel',\n 'bert/encoder/layer_11/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_11/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_11/attention/self/query/bias',\n 'bert/encoder/layer_11/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_11/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_11/attention/self/query/kernel',\n 'bert/encoder/layer_11/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_11/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_11/attention/self/value/bias',\n 'bert/encoder/layer_11/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_11/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_11/attention/self/value/kernel',\n 'bert/encoder/layer_11/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_11/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_11/intermediate/dense/bias',\n 'bert/encoder/layer_11/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_11/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_11/intermediate/dense/kernel',\n 'bert/encoder/layer_11/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_11/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_11/output/LayerNorm/beta',\n 'bert/encoder/layer_11/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_11/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_11/output/LayerNorm/gamma',\n 'bert/encoder/layer_11/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_11/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_11/output/dense/bias',\n 'bert/encoder/layer_11/output/dense/bias/adam_m',\n 'bert/encoder/layer_11/output/dense/bias/adam_v',\n 'bert/encoder/layer_11/output/dense/kernel',\n 'bert/encoder/layer_11/output/dense/kernel/adam_m',\n 'bert/encoder/layer_11/output/dense/kernel/adam_v',\n 'bert/encoder/layer_12/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_12/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_12/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_12/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_12/attention/output/dense/bias',\n 'bert/encoder/layer_12/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_12/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_12/attention/output/dense/kernel',\n 'bert/encoder/layer_12/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_12/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_12/attention/self/key/bias',\n 'bert/encoder/layer_12/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_12/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_12/attention/self/key/kernel',\n 'bert/encoder/layer_12/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_12/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_12/attention/self/query/bias',\n 'bert/encoder/layer_12/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_12/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_12/attention/self/query/kernel',\n 'bert/encoder/layer_12/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_12/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_12/attention/self/value/bias',\n 'bert/encoder/layer_12/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_12/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_12/attention/self/value/kernel',\n 'bert/encoder/layer_12/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_12/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_12/intermediate/dense/bias',\n 'bert/encoder/layer_12/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_12/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_12/intermediate/dense/kernel',\n 'bert/encoder/layer_12/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_12/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_12/output/LayerNorm/beta',\n 'bert/encoder/layer_12/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_12/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_12/output/LayerNorm/gamma',\n 'bert/encoder/layer_12/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_12/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_12/output/dense/bias',\n 'bert/encoder/layer_12/output/dense/bias/adam_m',\n 'bert/encoder/layer_12/output/dense/bias/adam_v',\n 'bert/encoder/layer_12/output/dense/kernel',\n 'bert/encoder/layer_12/output/dense/kernel/adam_m',\n 'bert/encoder/layer_12/output/dense/kernel/adam_v',\n 'bert/encoder/layer_13/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_13/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_13/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_13/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_13/attention/output/dense/bias',\n 'bert/encoder/layer_13/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_13/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_13/attention/output/dense/kernel',\n 'bert/encoder/layer_13/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_13/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_13/attention/self/key/bias',\n 'bert/encoder/layer_13/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_13/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_13/attention/self/key/kernel',\n 'bert/encoder/layer_13/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_13/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_13/attention/self/query/bias',\n 'bert/encoder/layer_13/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_13/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_13/attention/self/query/kernel',\n 'bert/encoder/layer_13/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_13/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_13/attention/self/value/bias',\n 'bert/encoder/layer_13/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_13/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_13/attention/self/value/kernel',\n 'bert/encoder/layer_13/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_13/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_13/intermediate/dense/bias',\n 'bert/encoder/layer_13/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_13/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_13/intermediate/dense/kernel',\n 'bert/encoder/layer_13/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_13/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_13/output/LayerNorm/beta',\n 'bert/encoder/layer_13/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_13/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_13/output/LayerNorm/gamma',\n 'bert/encoder/layer_13/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_13/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_13/output/dense/bias',\n 'bert/encoder/layer_13/output/dense/bias/adam_m',\n 'bert/encoder/layer_13/output/dense/bias/adam_v',\n 'bert/encoder/layer_13/output/dense/kernel',\n 'bert/encoder/layer_13/output/dense/kernel/adam_m',\n 'bert/encoder/layer_13/output/dense/kernel/adam_v',\n 'bert/encoder/layer_14/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_14/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_14/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_14/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_14/attention/output/dense/bias',\n 'bert/encoder/layer_14/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_14/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_14/attention/output/dense/kernel',\n 'bert/encoder/layer_14/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_14/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_14/attention/self/key/bias',\n 'bert/encoder/layer_14/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_14/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_14/attention/self/key/kernel',\n 'bert/encoder/layer_14/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_14/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_14/attention/self/query/bias',\n 'bert/encoder/layer_14/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_14/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_14/attention/self/query/kernel',\n 'bert/encoder/layer_14/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_14/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_14/attention/self/value/bias',\n 'bert/encoder/layer_14/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_14/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_14/attention/self/value/kernel',\n 'bert/encoder/layer_14/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_14/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_14/intermediate/dense/bias',\n 'bert/encoder/layer_14/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_14/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_14/intermediate/dense/kernel',\n 'bert/encoder/layer_14/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_14/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_14/output/LayerNorm/beta',\n 'bert/encoder/layer_14/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_14/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_14/output/LayerNorm/gamma',\n 'bert/encoder/layer_14/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_14/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_14/output/dense/bias',\n 'bert/encoder/layer_14/output/dense/bias/adam_m',\n 'bert/encoder/layer_14/output/dense/bias/adam_v',\n 'bert/encoder/layer_14/output/dense/kernel',\n 'bert/encoder/layer_14/output/dense/kernel/adam_m',\n 'bert/encoder/layer_14/output/dense/kernel/adam_v',\n 'bert/encoder/layer_15/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_15/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_15/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_15/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_15/attention/output/dense/bias',\n 'bert/encoder/layer_15/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_15/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_15/attention/output/dense/kernel',\n 'bert/encoder/layer_15/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_15/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_15/attention/self/key/bias',\n 'bert/encoder/layer_15/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_15/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_15/attention/self/key/kernel',\n 'bert/encoder/layer_15/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_15/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_15/attention/self/query/bias',\n 'bert/encoder/layer_15/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_15/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_15/attention/self/query/kernel',\n 'bert/encoder/layer_15/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_15/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_15/attention/self/value/bias',\n 'bert/encoder/layer_15/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_15/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_15/attention/self/value/kernel',\n 'bert/encoder/layer_15/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_15/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_15/intermediate/dense/bias',\n 'bert/encoder/layer_15/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_15/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_15/intermediate/dense/kernel',\n 'bert/encoder/layer_15/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_15/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_15/output/LayerNorm/beta',\n 'bert/encoder/layer_15/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_15/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_15/output/LayerNorm/gamma',\n 'bert/encoder/layer_15/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_15/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_15/output/dense/bias',\n 'bert/encoder/layer_15/output/dense/bias/adam_m',\n 'bert/encoder/layer_15/output/dense/bias/adam_v',\n 'bert/encoder/layer_15/output/dense/kernel',\n 'bert/encoder/layer_15/output/dense/kernel/adam_m',\n 'bert/encoder/layer_15/output/dense/kernel/adam_v',\n 'bert/encoder/layer_16/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_16/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_16/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_16/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_16/attention/output/dense/bias',\n 'bert/encoder/layer_16/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_16/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_16/attention/output/dense/kernel',\n 'bert/encoder/layer_16/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_16/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_16/attention/self/key/bias',\n 'bert/encoder/layer_16/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_16/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_16/attention/self/key/kernel',\n 'bert/encoder/layer_16/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_16/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_16/attention/self/query/bias',\n 'bert/encoder/layer_16/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_16/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_16/attention/self/query/kernel',\n 'bert/encoder/layer_16/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_16/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_16/attention/self/value/bias',\n 'bert/encoder/layer_16/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_16/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_16/attention/self/value/kernel',\n 'bert/encoder/layer_16/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_16/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_16/intermediate/dense/bias',\n 'bert/encoder/layer_16/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_16/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_16/intermediate/dense/kernel',\n 'bert/encoder/layer_16/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_16/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_16/output/LayerNorm/beta',\n 'bert/encoder/layer_16/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_16/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_16/output/LayerNorm/gamma',\n 'bert/encoder/layer_16/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_16/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_16/output/dense/bias',\n 'bert/encoder/layer_16/output/dense/bias/adam_m',\n 'bert/encoder/layer_16/output/dense/bias/adam_v',\n 'bert/encoder/layer_16/output/dense/kernel',\n 'bert/encoder/layer_16/output/dense/kernel/adam_m',\n 'bert/encoder/layer_16/output/dense/kernel/adam_v',\n 'bert/encoder/layer_17/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_17/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_17/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_17/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_17/attention/output/dense/bias',\n 'bert/encoder/layer_17/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_17/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_17/attention/output/dense/kernel',\n 'bert/encoder/layer_17/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_17/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_17/attention/self/key/bias',\n 'bert/encoder/layer_17/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_17/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_17/attention/self/key/kernel',\n 'bert/encoder/layer_17/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_17/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_17/attention/self/query/bias',\n 'bert/encoder/layer_17/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_17/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_17/attention/self/query/kernel',\n 'bert/encoder/layer_17/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_17/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_17/attention/self/value/bias',\n 'bert/encoder/layer_17/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_17/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_17/attention/self/value/kernel',\n 'bert/encoder/layer_17/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_17/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_17/intermediate/dense/bias',\n 'bert/encoder/layer_17/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_17/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_17/intermediate/dense/kernel',\n 'bert/encoder/layer_17/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_17/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_17/output/LayerNorm/beta',\n 'bert/encoder/layer_17/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_17/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_17/output/LayerNorm/gamma',\n 'bert/encoder/layer_17/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_17/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_17/output/dense/bias',\n 'bert/encoder/layer_17/output/dense/bias/adam_m',\n 'bert/encoder/layer_17/output/dense/bias/adam_v',\n 'bert/encoder/layer_17/output/dense/kernel',\n 'bert/encoder/layer_17/output/dense/kernel/adam_m',\n 'bert/encoder/layer_17/output/dense/kernel/adam_v',\n 'bert/encoder/layer_18/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_18/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_18/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_18/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_18/attention/output/dense/bias',\n 'bert/encoder/layer_18/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_18/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_18/attention/output/dense/kernel',\n 'bert/encoder/layer_18/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_18/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_18/attention/self/key/bias',\n 'bert/encoder/layer_18/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_18/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_18/attention/self/key/kernel',\n 'bert/encoder/layer_18/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_18/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_18/attention/self/query/bias',\n 'bert/encoder/layer_18/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_18/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_18/attention/self/query/kernel',\n 'bert/encoder/layer_18/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_18/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_18/attention/self/value/bias',\n 'bert/encoder/layer_18/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_18/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_18/attention/self/value/kernel',\n 'bert/encoder/layer_18/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_18/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_18/intermediate/dense/bias',\n 'bert/encoder/layer_18/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_18/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_18/intermediate/dense/kernel',\n 'bert/encoder/layer_18/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_18/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_18/output/LayerNorm/beta',\n 'bert/encoder/layer_18/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_18/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_18/output/LayerNorm/gamma',\n 'bert/encoder/layer_18/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_18/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_18/output/dense/bias',\n 'bert/encoder/layer_18/output/dense/bias/adam_m',\n 'bert/encoder/layer_18/output/dense/bias/adam_v',\n 'bert/encoder/layer_18/output/dense/kernel',\n 'bert/encoder/layer_18/output/dense/kernel/adam_m',\n 'bert/encoder/layer_18/output/dense/kernel/adam_v',\n 'bert/encoder/layer_19/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_19/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_19/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_19/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_19/attention/output/dense/bias',\n 'bert/encoder/layer_19/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_19/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_19/attention/output/dense/kernel',\n 'bert/encoder/layer_19/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_19/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_19/attention/self/key/bias',\n 'bert/encoder/layer_19/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_19/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_19/attention/self/key/kernel',\n 'bert/encoder/layer_19/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_19/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_19/attention/self/query/bias',\n 'bert/encoder/layer_19/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_19/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_19/attention/self/query/kernel',\n 'bert/encoder/layer_19/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_19/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_19/attention/self/value/bias',\n 'bert/encoder/layer_19/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_19/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_19/attention/self/value/kernel',\n 'bert/encoder/layer_19/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_19/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_19/intermediate/dense/bias',\n 'bert/encoder/layer_19/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_19/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_19/intermediate/dense/kernel',\n 'bert/encoder/layer_19/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_19/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_19/output/LayerNorm/beta',\n 'bert/encoder/layer_19/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_19/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_19/output/LayerNorm/gamma',\n 'bert/encoder/layer_19/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_19/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_19/output/dense/bias',\n 'bert/encoder/layer_19/output/dense/bias/adam_m',\n 'bert/encoder/layer_19/output/dense/bias/adam_v',\n 'bert/encoder/layer_19/output/dense/kernel',\n 'bert/encoder/layer_19/output/dense/kernel/adam_m',\n 'bert/encoder/layer_19/output/dense/kernel/adam_v',\n 'bert/encoder/layer_2/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_2/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_2/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_2/attention/output/dense/bias',\n 'bert/encoder/layer_2/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_2/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_2/attention/output/dense/kernel',\n 'bert/encoder/layer_2/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_2/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_2/attention/self/key/bias',\n 'bert/encoder/layer_2/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_2/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_2/attention/self/key/kernel',\n 'bert/encoder/layer_2/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_2/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_2/attention/self/query/bias',\n 'bert/encoder/layer_2/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_2/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_2/attention/self/query/kernel',\n 'bert/encoder/layer_2/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_2/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_2/attention/self/value/bias',\n 'bert/encoder/layer_2/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_2/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_2/attention/self/value/kernel',\n 'bert/encoder/layer_2/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_2/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_2/intermediate/dense/bias',\n 'bert/encoder/layer_2/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_2/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_2/intermediate/dense/kernel',\n 'bert/encoder/layer_2/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_2/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_2/output/LayerNorm/beta',\n 'bert/encoder/layer_2/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_2/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_2/output/LayerNorm/gamma',\n 'bert/encoder/layer_2/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_2/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_2/output/dense/bias',\n 'bert/encoder/layer_2/output/dense/bias/adam_m',\n 'bert/encoder/layer_2/output/dense/bias/adam_v',\n 'bert/encoder/layer_2/output/dense/kernel',\n 'bert/encoder/layer_2/output/dense/kernel/adam_m',\n 'bert/encoder/layer_2/output/dense/kernel/adam_v',\n 'bert/encoder/layer_20/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_20/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_20/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_20/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_20/attention/output/dense/bias',\n 'bert/encoder/layer_20/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_20/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_20/attention/output/dense/kernel',\n 'bert/encoder/layer_20/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_20/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_20/attention/self/key/bias',\n 'bert/encoder/layer_20/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_20/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_20/attention/self/key/kernel',\n 'bert/encoder/layer_20/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_20/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_20/attention/self/query/bias',\n 'bert/encoder/layer_20/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_20/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_20/attention/self/query/kernel',\n 'bert/encoder/layer_20/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_20/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_20/attention/self/value/bias',\n 'bert/encoder/layer_20/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_20/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_20/attention/self/value/kernel',\n 'bert/encoder/layer_20/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_20/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_20/intermediate/dense/bias',\n 'bert/encoder/layer_20/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_20/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_20/intermediate/dense/kernel',\n 'bert/encoder/layer_20/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_20/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_20/output/LayerNorm/beta',\n 'bert/encoder/layer_20/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_20/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_20/output/LayerNorm/gamma',\n 'bert/encoder/layer_20/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_20/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_20/output/dense/bias',\n 'bert/encoder/layer_20/output/dense/bias/adam_m',\n 'bert/encoder/layer_20/output/dense/bias/adam_v',\n 'bert/encoder/layer_20/output/dense/kernel',\n 'bert/encoder/layer_20/output/dense/kernel/adam_m',\n 'bert/encoder/layer_20/output/dense/kernel/adam_v',\n 'bert/encoder/layer_21/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_21/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_21/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_21/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_21/attention/output/dense/bias',\n 'bert/encoder/layer_21/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_21/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_21/attention/output/dense/kernel',\n 'bert/encoder/layer_21/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_21/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_21/attention/self/key/bias',\n 'bert/encoder/layer_21/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_21/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_21/attention/self/key/kernel',\n 'bert/encoder/layer_21/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_21/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_21/attention/self/query/bias',\n 'bert/encoder/layer_21/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_21/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_21/attention/self/query/kernel',\n 'bert/encoder/layer_21/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_21/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_21/attention/self/value/bias',\n 'bert/encoder/layer_21/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_21/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_21/attention/self/value/kernel',\n 'bert/encoder/layer_21/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_21/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_21/intermediate/dense/bias',\n 'bert/encoder/layer_21/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_21/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_21/intermediate/dense/kernel',\n 'bert/encoder/layer_21/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_21/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_21/output/LayerNorm/beta',\n 'bert/encoder/layer_21/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_21/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_21/output/LayerNorm/gamma',\n 'bert/encoder/layer_21/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_21/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_21/output/dense/bias',\n 'bert/encoder/layer_21/output/dense/bias/adam_m',\n 'bert/encoder/layer_21/output/dense/bias/adam_v',\n 'bert/encoder/layer_21/output/dense/kernel',\n 'bert/encoder/layer_21/output/dense/kernel/adam_m',\n 'bert/encoder/layer_21/output/dense/kernel/adam_v',\n 'bert/encoder/layer_22/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_22/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_22/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_22/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_22/attention/output/dense/bias',\n 'bert/encoder/layer_22/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_22/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_22/attention/output/dense/kernel',\n 'bert/encoder/layer_22/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_22/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_22/attention/self/key/bias',\n 'bert/encoder/layer_22/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_22/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_22/attention/self/key/kernel',\n 'bert/encoder/layer_22/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_22/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_22/attention/self/query/bias',\n 'bert/encoder/layer_22/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_22/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_22/attention/self/query/kernel',\n 'bert/encoder/layer_22/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_22/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_22/attention/self/value/bias',\n 'bert/encoder/layer_22/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_22/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_22/attention/self/value/kernel',\n 'bert/encoder/layer_22/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_22/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_22/intermediate/dense/bias',\n 'bert/encoder/layer_22/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_22/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_22/intermediate/dense/kernel',\n 'bert/encoder/layer_22/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_22/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_22/output/LayerNorm/beta',\n 'bert/encoder/layer_22/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_22/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_22/output/LayerNorm/gamma',\n 'bert/encoder/layer_22/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_22/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_22/output/dense/bias',\n 'bert/encoder/layer_22/output/dense/bias/adam_m',\n 'bert/encoder/layer_22/output/dense/bias/adam_v',\n 'bert/encoder/layer_22/output/dense/kernel',\n 'bert/encoder/layer_22/output/dense/kernel/adam_m',\n 'bert/encoder/layer_22/output/dense/kernel/adam_v',\n 'bert/encoder/layer_23/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_23/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_23/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_23/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_23/attention/output/dense/bias',\n 'bert/encoder/layer_23/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_23/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_23/attention/output/dense/kernel',\n 'bert/encoder/layer_23/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_23/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_23/attention/self/key/bias',\n 'bert/encoder/layer_23/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_23/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_23/attention/self/key/kernel',\n 'bert/encoder/layer_23/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_23/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_23/attention/self/query/bias',\n 'bert/encoder/layer_23/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_23/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_23/attention/self/query/kernel',\n 'bert/encoder/layer_23/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_23/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_23/attention/self/value/bias',\n 'bert/encoder/layer_23/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_23/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_23/attention/self/value/kernel',\n 'bert/encoder/layer_23/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_23/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_23/intermediate/dense/bias',\n 'bert/encoder/layer_23/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_23/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_23/intermediate/dense/kernel',\n 'bert/encoder/layer_23/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_23/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_23/output/LayerNorm/beta',\n 'bert/encoder/layer_23/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_23/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_23/output/LayerNorm/gamma',\n 'bert/encoder/layer_23/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_23/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_23/output/dense/bias',\n 'bert/encoder/layer_23/output/dense/bias/adam_m',\n 'bert/encoder/layer_23/output/dense/bias/adam_v',\n 'bert/encoder/layer_23/output/dense/kernel',\n 'bert/encoder/layer_23/output/dense/kernel/adam_m',\n 'bert/encoder/layer_23/output/dense/kernel/adam_v',\n 'bert/encoder/layer_3/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_3/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_3/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_3/attention/output/dense/bias',\n 'bert/encoder/layer_3/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_3/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_3/attention/output/dense/kernel',\n 'bert/encoder/layer_3/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_3/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_3/attention/self/key/bias',\n 'bert/encoder/layer_3/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_3/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_3/attention/self/key/kernel',\n 'bert/encoder/layer_3/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_3/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_3/attention/self/query/bias',\n 'bert/encoder/layer_3/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_3/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_3/attention/self/query/kernel',\n 'bert/encoder/layer_3/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_3/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_3/attention/self/value/bias',\n 'bert/encoder/layer_3/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_3/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_3/attention/self/value/kernel',\n 'bert/encoder/layer_3/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_3/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_3/intermediate/dense/bias',\n 'bert/encoder/layer_3/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_3/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_3/intermediate/dense/kernel',\n 'bert/encoder/layer_3/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_3/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_3/output/LayerNorm/beta',\n 'bert/encoder/layer_3/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_3/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_3/output/LayerNorm/gamma',\n 'bert/encoder/layer_3/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_3/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_3/output/dense/bias',\n 'bert/encoder/layer_3/output/dense/bias/adam_m',\n 'bert/encoder/layer_3/output/dense/bias/adam_v',\n 'bert/encoder/layer_3/output/dense/kernel',\n 'bert/encoder/layer_3/output/dense/kernel/adam_m',\n 'bert/encoder/layer_3/output/dense/kernel/adam_v',\n 'bert/encoder/layer_4/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_4/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_4/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_4/attention/output/dense/bias',\n 'bert/encoder/layer_4/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_4/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_4/attention/output/dense/kernel',\n 'bert/encoder/layer_4/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_4/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_4/attention/self/key/bias',\n 'bert/encoder/layer_4/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_4/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_4/attention/self/key/kernel',\n 'bert/encoder/layer_4/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_4/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_4/attention/self/query/bias',\n 'bert/encoder/layer_4/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_4/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_4/attention/self/query/kernel',\n 'bert/encoder/layer_4/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_4/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_4/attention/self/value/bias',\n 'bert/encoder/layer_4/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_4/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_4/attention/self/value/kernel',\n 'bert/encoder/layer_4/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_4/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_4/intermediate/dense/bias',\n 'bert/encoder/layer_4/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_4/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_4/intermediate/dense/kernel',\n 'bert/encoder/layer_4/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_4/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_4/output/LayerNorm/beta',\n 'bert/encoder/layer_4/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_4/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_4/output/LayerNorm/gamma',\n 'bert/encoder/layer_4/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_4/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_4/output/dense/bias',\n 'bert/encoder/layer_4/output/dense/bias/adam_m',\n 'bert/encoder/layer_4/output/dense/bias/adam_v',\n 'bert/encoder/layer_4/output/dense/kernel',\n 'bert/encoder/layer_4/output/dense/kernel/adam_m',\n 'bert/encoder/layer_4/output/dense/kernel/adam_v',\n 'bert/encoder/layer_5/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_5/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_5/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_5/attention/output/dense/bias',\n 'bert/encoder/layer_5/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_5/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_5/attention/output/dense/kernel',\n 'bert/encoder/layer_5/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_5/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_5/attention/self/key/bias',\n 'bert/encoder/layer_5/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_5/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_5/attention/self/key/kernel',\n 'bert/encoder/layer_5/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_5/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_5/attention/self/query/bias',\n 'bert/encoder/layer_5/attention/self/query/bias/adam_m',\n 'bert/encoder/layer_5/attention/self/query/bias/adam_v',\n 'bert/encoder/layer_5/attention/self/query/kernel',\n 'bert/encoder/layer_5/attention/self/query/kernel/adam_m',\n 'bert/encoder/layer_5/attention/self/query/kernel/adam_v',\n 'bert/encoder/layer_5/attention/self/value/bias',\n 'bert/encoder/layer_5/attention/self/value/bias/adam_m',\n 'bert/encoder/layer_5/attention/self/value/bias/adam_v',\n 'bert/encoder/layer_5/attention/self/value/kernel',\n 'bert/encoder/layer_5/attention/self/value/kernel/adam_m',\n 'bert/encoder/layer_5/attention/self/value/kernel/adam_v',\n 'bert/encoder/layer_5/intermediate/dense/bias',\n 'bert/encoder/layer_5/intermediate/dense/bias/adam_m',\n 'bert/encoder/layer_5/intermediate/dense/bias/adam_v',\n 'bert/encoder/layer_5/intermediate/dense/kernel',\n 'bert/encoder/layer_5/intermediate/dense/kernel/adam_m',\n 'bert/encoder/layer_5/intermediate/dense/kernel/adam_v',\n 'bert/encoder/layer_5/output/LayerNorm/beta',\n 'bert/encoder/layer_5/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_5/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_5/output/LayerNorm/gamma',\n 'bert/encoder/layer_5/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_5/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_5/output/dense/bias',\n 'bert/encoder/layer_5/output/dense/bias/adam_m',\n 'bert/encoder/layer_5/output/dense/bias/adam_v',\n 'bert/encoder/layer_5/output/dense/kernel',\n 'bert/encoder/layer_5/output/dense/kernel/adam_m',\n 'bert/encoder/layer_5/output/dense/kernel/adam_v',\n 'bert/encoder/layer_6/attention/output/LayerNorm/beta',\n 'bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_m',\n 'bert/encoder/layer_6/attention/output/LayerNorm/beta/adam_v',\n 'bert/encoder/layer_6/attention/output/LayerNorm/gamma',\n 'bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m',\n 'bert/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v',\n 'bert/encoder/layer_6/attention/output/dense/bias',\n 'bert/encoder/layer_6/attention/output/dense/bias/adam_m',\n 'bert/encoder/layer_6/attention/output/dense/bias/adam_v',\n 'bert/encoder/layer_6/attention/output/dense/kernel',\n 'bert/encoder/layer_6/attention/output/dense/kernel/adam_m',\n 'bert/encoder/layer_6/attention/output/dense/kernel/adam_v',\n 'bert/encoder/layer_6/attention/self/key/bias',\n 'bert/encoder/layer_6/attention/self/key/bias/adam_m',\n 'bert/encoder/layer_6/attention/self/key/bias/adam_v',\n 'bert/encoder/layer_6/attention/self/key/kernel',\n 'bert/encoder/layer_6/attention/self/key/kernel/adam_m',\n 'bert/encoder/layer_6/attention/self/key/kernel/adam_v',\n 'bert/encoder/layer_6/attention/self/query/bias',\n ...]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Model: \"bert-baseline\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModel)                ((None, 1024), (None 335141888   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "logits (TDense)                 (None, 512, 2)       2050        bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 512, 1), (No 0           logits[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "unique_id (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_squeeze (Tens [(None, 512)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_squeeze (Tensor [(None, 512)]        0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "ans_type (TDense)               (None, 5)            5125        bert[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 335,149,063\n",
      "Trainable params: 335,149,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "['ans_type' 'bert' 'logits']\n"
     ],
     "output_type": "stream"
    },
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e9de69673664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0ma_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_dir_or_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCPKT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massignment_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mcpkt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36minit_from_checkpoint\u001b[0;34m(ckpt_dir_or_file, assignment_map)\u001b[0m\n\u001b[1;32m    286\u001b[0m       ckpt_dir_or_file, assignment_map)\n\u001b[1;32m    287\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cross_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m     \u001b[0minit_from_checkpoint_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     distribution_strategy_context.get_replica_context().merge_call(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m    284\u001b[0m   \"\"\"\n\u001b[1;32m    285\u001b[0m   init_from_checkpoint_fn = lambda _: _init_from_checkpoint(\n\u001b[0;32m--> 286\u001b[0;31m       ckpt_dir_or_file, assignment_map)\n\u001b[0m\u001b[1;32m    287\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cross_replica_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0minit_from_checkpoint_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36m_init_from_checkpoint\u001b[0;34m(ckpt_dir_or_file, assignment_map)\u001b[0m\n\u001b[1;32m    312\u001b[0m       \u001b[0;31m# Also check if variable is partitioned as list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collect_partitioned_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_var_or_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0;31m# If 1 to 1 mapping was provided, find variable in the checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/training/checkpoint_utils.py\u001b[0m in \u001b[0;36m_collect_partitioned_variable\u001b[0;34m(name, all_vars)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_collect_partitioned_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m   \u001b[0;34m\"\"\"Returns list of `tf.Variable` that comprise the partitioned variable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/part_0\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m     \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ],
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error"
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = mk_model(config)\n",
    "    model.summary()\n",
    "    \n",
    "    model_params = {v.name:v for v in model.trainable_variables}\n",
    "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
    "    print(model_roots)\n",
    "    saved_names = [k for k,v in tf.train.list_variables(CPKT_PATH)]\n",
    "    a_map = {v:v+':0' for v in saved_names}\n",
    "    model_roots = np.unique([v.name.split('/')[0] for v in model.trainable_variables])\n",
    "    def transform(x):\n",
    "        x = x.replace('attention/self','attention')\n",
    "        x = x.replace('attention','self_attention')\n",
    "        x = x.replace('attention/output','attention_output')  \n",
    "\n",
    "        x = x.replace('/dense','')\n",
    "        x = x.replace('/LayerNorm','_layer_norm')\n",
    "        x = x.replace('embeddings_layer_norm','embeddings/layer_norm')  \n",
    "\n",
    "        x = x.replace('attention_output_layer_norm','attention_layer_norm')  \n",
    "        x = x.replace('embeddings/word_embeddings','word_embeddings/embeddings')\n",
    "\n",
    "        x = x.replace('/embeddings/','/embedding_postprocessor/')  \n",
    "        x = x.replace('/token_type_embeddings','/type_embeddings')  \n",
    "        x = x.replace('/pooler/','/pooler_transform/')  \n",
    "        x = x.replace('answer_type_output_bias','ans_type/bias')  \n",
    "        x = x.replace('answer_type_output_','ans_type/')\n",
    "        x = x.replace('cls/nq/output_','logits/')\n",
    "        x = x.replace('/weights','/kernel')\n",
    "\n",
    "        return x\n",
    "    a_map = {k:model_params.get(transform(v),None) for k,v in a_map.items() if k!='global_step'}\n",
    "    tf.compat.v1.train.init_from_checkpoint(ckpt_dir_or_file=CPKT_PATH, assignment_map=a_map)\n",
    "    \n",
    "    cpkt = tf.train.Checkpoint(model=model)\n",
    "    cpkt.restore(CPKT_PATH).assert_consumed()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# small_config = config.copy()\n",
    "# small_config['vocab_size']=16\n",
    "# small_config['hidden_size']=64\n",
    "# small_config['max_position_embeddings'] = 32\n",
    "# small_config['num_hidden_layers'] = 4\n",
    "# small_config['num_attention_heads'] = 4\n",
    "# small_config['intermediate_size'] = 256\n",
    "# small_config\n",
    "\n",
    "tf.cast\n",
    "\n",
    "tf.squeeze\n",
    "\n",
    "model.save_weights()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not url_exists(NQ_TEST_TFRECORD_PATH):\n",
    "# if True:\n",
    "    # tf2baseline.FLAGS.max_seq_length = 512\n",
    "    eval_writer = bert_utils.FeatureWriter(filename=NQ_TEST_TFRECORD_PATH,\n",
    "                                           is_training=False)\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_PATH,\n",
    "                                           do_lower_case=True)\n",
    "    features = []\n",
    "    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n",
    "                                                  is_training=False,\n",
    "                                                  output_fn=eval_writer.process_feature,\n",
    "                                                  collect_stat=False)\n",
    "    n_examples = 0\n",
    "    # tqdm_notebook = tqdm.tqdm_notebook  # if not on_kaggle_server else None\n",
    "    for examples in bert_utils.nq_examples_iter(input_file=NQ_TEST_JSONL_PATH,\n",
    "                                                is_training=False,\n",
    "                                                tqdm=tqdm):\n",
    "        for example in examples:\n",
    "            n_examples += convert(example)\n",
    "    eval_writer.close()\n",
    "    print('number of test examples: %d, written to file: %d' % (n_examples, eval_writer.num_features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "raw_ds = tf.data.TFRecordDataset(NQ_TEST_TFRECORD_PATH)\n",
    "decoded_ds = raw_ds.map(_decode_record)\n",
    "batched_ds = decoded_ds.batch(batch_size=BATCH_SIZE, drop_remainder=(TPU is not None))\n",
    "\n",
    "result = model.predict(batched_ds, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add example_id to beginning.\n",
    "example_id_ds = raw_ds.map(lambda x: tf.io.parse_single_example(\n",
    "    serialized=x,\n",
    "    features={\"example_id\": tf.io.FixedLenFeature([], tf.int64)}\n",
    ")['example_id'])\n",
    "result = (np.array(list(example_id_ds)[:len(result[0])]), *result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1- Understanding the code\n",
    "#### For a better understanding, I will briefly explain here.\n",
    "#### In the item \"answer_type\", in the last lines of this block, it is responsible for storing the identified response type, which, according to [github project repository](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py) can be:\n",
    "UNKNOWN = 0\n",
    "YES = 1\n",
    "NO = 2\n",
    "SHORT = 3\n",
    "LONG = 4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"getting candidates...\")\n",
    "candidates_dict = read_candidates('../input/tensorflow2-question-answering/simplified-nq-test.jsonl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"getting result_df...\")\n",
    "result_df = pd.DataFrame({\n",
    "    \"example_id\": result[0].squeeze().tolist(),\n",
    "    \"unique_id\": result[1].squeeze().tolist(),\n",
    "    \"start_logits\": result[2].tolist(),\n",
    "    \"end_logits\": result[3].tolist(),\n",
    "    \"answer_type_logits\": result[4].tolist()\n",
    "}).set_index(['example_id', 'unique_id'])\n",
    "# we pad some instances when using TPU, deduplicate it here.\n",
    "if TPU is not None:\n",
    "    print('result_df len before dedup: ' + str(len(result_df)))\n",
    "    result_df = result_df[~result_df.index.duplicated()]\n",
    "    print('result_df len after  dedup: ' + str(len(result_df)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "token_map_ds = raw_ds.map(lambda x: tf.io.parse_single_example(\n",
    "    serialized=x,\n",
    "    features={\n",
    "        \"example_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        # token_map: token to origin map.\n",
    "        \"token_map\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64)\n",
    "    }\n",
    "))\n",
    "print(\"getting token_map_df...\")\n",
    "token_map_df = pd.DataFrame.from_records(list(token_map_ds)).applymap(\n",
    "    lambda x: x.numpy()\n",
    ").set_index(['example_id', 'unique_id'])\n",
    "# we pad some instances when using TPU, deduplicate it here.\n",
    "if TPU is not None:\n",
    "    print('token_map_df len before: ' + str(len(token_map_df)))\n",
    "    token_map_df = token_map_df[~token_map_df.index.duplicated()]\n",
    "    print('token_map_df len before: ' + str(len(token_map_df)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "joined = result_df.join(token_map_df, on=['example_id', 'unique_id'])\n",
    "\n",
    "pred_df = pd.DataFrame(columns=['example_id', 'score', 'answer_type',\n",
    "                                'short_span_start', 'short_span_end',\n",
    "                                'long_span_start', 'long_span_end', ]\n",
    "                       ).set_index('example_id')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def best_score_start_end_of_instance(res: pd.Series):\n",
    "    \"\"\"\n",
    "    :param res: index: ['answer_type_logits', 'end_logits', 'start_logits', 'token_map', 'candidates']\n",
    "    :return: best_score_of_instance, start_short_idx, end_short_idx\n",
    "    \"\"\"\n",
    "    msk_invalid_token = np.array(res['token_map']) == -1\n",
    "    s_logits, e_logits = pd.Series(res['start_logits']), pd.Series(res['end_logits'])\n",
    "    # filter logits corresponding to context token and rank top-k.\n",
    "    s_msk_not_top_k = s_logits.mask(msk_invalid_token) \\\n",
    "                          .rank(method='min', ascending=False) > FLAGS.n_best_size\n",
    "    s_indexes = np.ma.masked_array(np.arange(s_logits.size),\n",
    "                                   mask=s_msk_not_top_k | msk_invalid_token)\n",
    "    e_msk_not_top_k = e_logits.mask(msk_invalid_token) \\\n",
    "                          .rank(method='min', ascending=False) > FLAGS.n_best_size\n",
    "    e_indexes = np.ma.masked_array(np.arange(e_logits.size),\n",
    "                                   mask=e_msk_not_top_k | msk_invalid_token)\n",
    "    # s_e_msk has shape: [512, 512], end index should greater than start index, otherwise, mask it.\n",
    "    s_e_msk = e_indexes[np.newaxis, :] <= s_indexes[:, np.newaxis]\n",
    "    # short answer length should litter than max_answer_length, otherwise, mask it.\n",
    "    s_e_msk |= (e_indexes[np.newaxis, :] - s_indexes[:, np.newaxis] >= FLAGS.max_answer_length)\n",
    "    # full mask.\n",
    "    s_e_msk = s_e_msk.filled(True)\n",
    "\n",
    "    if s_e_msk.all():  # if all start-end combinations has been masked.\n",
    "        return np.NAN, np.NAN, np.NAN\n",
    "    else:\n",
    "        # broadcast to shape: [512, 512], and set mask=s_e_msk\n",
    "        s_logits_bc = np.ma.array(\n",
    "            np.broadcast_to(s_logits[:, np.newaxis], shape=[s_logits.size, e_logits.size]),\n",
    "            mask=s_e_msk)\n",
    "        e_logits_bc = np.ma.array(\n",
    "            np.broadcast_to(e_logits[np.newaxis, :], shape=[s_logits.size, e_logits.size]),\n",
    "            mask=s_e_msk)\n",
    "        short_span_score = s_logits_bc + e_logits_bc\n",
    "        cls_token_score = s_logits[0] + e_logits[0]\n",
    "        score = short_span_score - cls_token_score\n",
    "        s_short_idx, e_short_idx = divmod(score.argmax(), e_logits.size)\n",
    "\n",
    "        return score.max(), s_short_idx, e_short_idx\n",
    "\n",
    "\n",
    "for example_id, group_df in tqdm(joined.groupby('example_id')):\n",
    "    # group_df: each row got a unique id(unique_id), all rows have a some example_id.\n",
    "    # columns = ['answer_type_logits', 'end_logits', 'start_logits', 'token_map', 'candidates']\n",
    "    group_df = group_df.copy().reset_index(level='example_id', drop=True)\n",
    "    # get best score/start/end and answer type for every instance within same example.\n",
    "    for u_id, res in group_df.iterrows():\n",
    "        answer_type_logits = pd.Series(res['answer_type_logits'], index=ANSWER_TYPE_ORDER)\n",
    "        group_df.loc[u_id, 'ins_answer_type'] = answer_type_logits.idxmax()\n",
    "        ins_score, ins_start, ins_end = best_score_start_end_of_instance(res)\n",
    "        group_df.loc[u_id, 'ins_score'] = ins_score\n",
    "        group_df.loc[u_id, 'ins_short_span_start'] = res['token_map'][ins_start]\n",
    "        # end span should be exclusive, and np.nan + 1 = np.nan\n",
    "        group_df.loc[u_id, 'ins_short_span_end'] = res['token_map'][ins_end] + 1\n",
    "    # we pick instance result who's best score is best among the instances within same example\n",
    "    best_u_id = group_df['ins_score'].idxmax()\n",
    "    if best_u_id is not np.NAN:  # if all instances got no score\n",
    "        short_span_start, short_span_end = group_df.loc[best_u_id, ['ins_short_span_start', 'ins_short_span_end']]\n",
    "        pred_df.loc[example_id, 'score'] = group_df.loc[best_u_id, 'ins_score']\n",
    "        pred_df.loc[example_id, 'short_span_start'] = short_span_start\n",
    "        pred_df.loc[example_id, 'short_span_end'] = short_span_end\n",
    "        # search for long answer span.\n",
    "        for cand in candidates_dict[str(example_id)]:\n",
    "            if cand['top_level'] and cand['start_token'] <= short_span_start and short_span_end <= cand['end_token']:\n",
    "                pred_df.loc[example_id, 'long_span_start'] = cand['start_token']\n",
    "                pred_df.loc[example_id, 'long_span_end'] = cand['end_token']\n",
    "                break\n",
    "        pred_df.loc[example_id, 'answer_type'] = group_df.loc[best_u_id, 'ins_answer_type']\n",
    "        # break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2- Main Change\n",
    "#### Here is the small, but main change: we created an if to check the predicted response type and thus filter / identify the responses that are passed to the submission file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering the Answers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_short_pred(pred_row: pd.Series):\n",
    "    # score(best short answer) is np.NAN means: there's no short/long answers.\n",
    "    if pred_row['score'] is np.NAN:\n",
    "        return ''\n",
    "    # answer_type can not be np.NAN if score is not np.NAN.\n",
    "    if pred_row['answer_type'] == 'UNKNOWN':\n",
    "        return ''\n",
    "    if pred_row['answer_type'] in ['YES', 'NO']:\n",
    "        return pred_row['answer_type']\n",
    "    if pred_row['answer_type'] in ['SHORT', 'LONG']:\n",
    "        if pred_row['score'] < 8:\n",
    "            return ''\n",
    "        else:\n",
    "            return '%d:%d' % (pred_row['short_span_start'], pred_row['short_span_end'])\n",
    "\n",
    "\n",
    "def get_long_pred(pred_row: pd.Series):\n",
    "    # score(best short answer) is np.NAN means: there's no short/long answers.\n",
    "    if pred_row['score'] is np.NAN:\n",
    "        return ''\n",
    "    # answer_type can not be np.NAN if score is not np.NAN.\n",
    "    if pred_row['answer_type'] == 'UNKNOWN':\n",
    "        return ''\n",
    "    if pred_row['answer_type'] in ['YES', 'NO', 'SHORT', 'LONG']:\n",
    "        if pred_row['score'] < 3 or pred_row['long_span_start'] is np.NAN:\n",
    "            return ''\n",
    "        else:\n",
    "            return '%d:%d' % (pred_row['long_span_start'], pred_row['long_span_end'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a DataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prediction_df = pred_df.copy()\n",
    "prediction_df['long_pred'] = pred_df.apply(get_long_pred, axis='columns')\n",
    "prediction_df['short_pred'] = pred_df.apply(get_short_pred, axis='columns')\n",
    "prediction_df.index = prediction_df.index.map(lambda x: str(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generating the Submission File"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH).set_index('example_id')\n",
    "\n",
    "for eid, row in prediction_df.iterrows():\n",
    "    sample_submission.loc[eid + '_long', 'PredictionString'] = row['long_pred']\n",
    "    sample_submission.loc[eid + '_short', 'PredictionString'] = row['short_pred']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_submission.reset_index().to_csv('submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sample_submission.head(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Yes\n",
    "Answers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "yes_answers = sample_submission[sample_submission['PredictionString'] == 'YES']\n",
    "yes_answers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No\n",
    "Answers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "no_answers = sample_submission[sample_submission['PredictionString'] == 'NO']\n",
    "no_answers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Balnk\n",
    "Answers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "blank_answers = sample_submission[sample_submission['PredictionString'] == '']\n",
    "blank_answers.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "blank_answers.count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### I am only sharing modifications that I believe may help. I left out Tunning and any significant code changes I made."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We'll be grateful if someone gets a better understanding and can share what really impacts the assessment. No need to share code, just knowledge.\n",
    "### Thank you!"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}