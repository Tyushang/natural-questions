{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "RUN_ON = 'kaggle' if os.path.exists('/kaggle') else 'gcp'\n",
    "\n",
    "if RUN_ON == 'gcp':\n",
    "    os.chdir('/home/jupyter/kaggle/working')\n",
    "    sys.path.extend(['../input/bert-joint-baseline/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import bert_utils\n",
    "import modeling\n",
    "import tokenization\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import importlib\n",
    "\n",
    "importlib.reload(bert_utils)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                            \"non-floating point (and non-complex) \"\n",
    "                            \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                             \"`TDense` should be defined. \"\n",
    "                             \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size, last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.matmul(x, self.kernel, transpose_b=True) + self.bias\n",
    "\n",
    "\n",
    "class DummyObject:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "\n",
    "def mk_model(config):\n",
    "    seq_len = config['max_position_embeddings']\n",
    "    unique_id = tf.keras.Input(shape=(1,), dtype=tf.int64, name='unique_id')\n",
    "    input_ids = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='input_ids')\n",
    "    input_mask = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(seq_len,), dtype=tf.int32, name='segment_ids')\n",
    "    BERT = modeling.BertModel(config=config, name='bert')\n",
    "    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n",
    "                                          input_mask=input_mask,\n",
    "                                          input_type_ids=segment_ids)\n",
    "\n",
    "    logits = TDense(2, name='logits')(sequence_output)\n",
    "    start_logits, end_logits = tf.split(logits, axis=-1, num_or_size_splits=2, name='split')\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1, name='start_squeeze')\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1, name='end_squeeze')\n",
    "\n",
    "    ans_type = TDense(5, name='ans_type')(pooled_output)\n",
    "    return tf.keras.Model([input_ for input_ in [unique_id, input_ids, input_mask, segment_ids]\n",
    "                           if input_ is not None],\n",
    "                          [unique_id, start_logits, end_logits, ans_type],\n",
    "                          name='bert-baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def url_exists(url):\n",
    "    \"\"\"test local or gs file exists or not.\"\"\"\n",
    "    from urllib import parse\n",
    "    res = parse.urlparse(url)\n",
    "    print(res)\n",
    "    if res.scheme == 'gs':\n",
    "        # blob_name has no '/' prefix\n",
    "        bucket_name, blob_name = res.netloc, res.path[1:]\n",
    "        from google.cloud import storage\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name[1:])\n",
    "        return blob.exists()\n",
    "    else:\n",
    "        return os.path.exists(res.path)\n",
    "\n",
    "\n",
    "def _decode_record(record, feature_description=None):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    feature_description = feature_description or FEATURE_DESCRIPTION\n",
    "    example = tf.io.parse_single_example(serialized=record, features=feature_description)\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for key in [k for k in example.keys() if k not in ['example_id', 'unique_id']]:\n",
    "        example[key] = tf.cast(example[key], dtype=tf.int32)\n",
    "    \n",
    "    example.pop('example_id')\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_candidates_from_one_split(input_path):\n",
    "    \"\"\"Read candidates from a single jsonl file.\"\"\"\n",
    "    candidates_dict = {}\n",
    "    print(\"Reading examples from: %s\" % input_path)\n",
    "    if input_path.endswith(\".gz\"):\n",
    "        with gzip.GzipFile(fileobj=tf.io.gfile.GFile(input_path, \"rb\")) as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "    else:\n",
    "        with tf.io.gfile.GFile(input_path, \"r\") as input_file:\n",
    "            for index, line in enumerate(input_file):\n",
    "                e = json.loads(line)\n",
    "                candidates_dict[e[\"example_id\"]] = e[\"long_answer_candidates\"]\n",
    "                # candidates_dict['question'] = e['question_text']\n",
    "    return candidates_dict\n",
    "\n",
    "\n",
    "def read_candidates(input_pattern):\n",
    "    \"\"\"Read candidates with real multiple processes.\"\"\"\n",
    "    input_paths = tf.io.gfile.glob(input_pattern)\n",
    "    final_dict = {}\n",
    "    for input_path in input_paths:\n",
    "        final_dict.update(read_candidates_from_one_split(input_path))\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"vocab_size\": 30522\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "FLAGS = DummyObject(skip_nested_contexts=True,\n",
    "                    max_position=50,\n",
    "                    max_contexts=48,\n",
    "                    max_query_length=64,\n",
    "                    max_seq_length=512,\n",
    "                    doc_stride=128,\n",
    "                    include_unknowns=-1.0,\n",
    "                    n_best_size=20,\n",
    "                    max_answer_length=30)\n",
    "\n",
    "SEQ_LENGTH = FLAGS.max_seq_length  # config['max_position_embeddings']\n",
    "\n",
    "if RUN_ON == 'gcp':\n",
    "    INPUT_PATH = 'gs://tyu-kaggle/input/'\n",
    "else:\n",
    "    INPUT_PATH = '../input/'\n",
    "BERT_CONFIG_PATH = os.path.join('../input', 'bert-joint-baseline/bert_config.json')\n",
    "CPKT_PATH = os.path.join(INPUT_PATH, 'bert-joint-baseline/model_cpkt-1')\n",
    "# CPKT_PATH = 'gs://tyu-kaggle/output/model.ckpt-23187.index'\n",
    "VOCAB_PATH = os.path.join(INPUT_PATH, 'bert-joint-baseline/vocab-nq.txt')\n",
    "\n",
    "NQ_TEST_JSONL_PATH = '../input/tensorflow2-question-answering/simplified-nq-test.jsonl'\n",
    "NQ_TRAIN_JSONL_PATH = '../input/tensorflow2-question-answering/simplified-nq-train.jsonl'\n",
    "NQ_TEST_TFRECORD_PATH = './nq-test.tfrecords'\n",
    "\n",
    "SAMPLE_SUBMISSION_PATH = '../input/tensorflow2-question-answering/sample_submission.csv'\n",
    "\n",
    "TEST_DS_TYPE = 'public' if os.path.getsize(NQ_TEST_JSONL_PATH) < 20000000 else 'private'\n",
    "\n",
    "FEATURE_DESCRIPTION = {\n",
    "    \"example_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "    \"input_ids\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "    \"input_mask\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "    \"segment_ids\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64),\n",
    "}\n",
    "ANSWER_TYPE_ORDER = ['UNKNOWN', 'YES', 'NO', 'SHORT', 'LONG']\n",
    "\n",
    "with open(BERT_CONFIG_PATH, 'r') as f:\n",
    "    config = json.load(f)\n",
    "print(json.dumps(config, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', TPU.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "    TPU = None\n",
    "\n",
    "if TPU:\n",
    "    tf.config.experimental_connect_to_cluster(TPU)\n",
    "    tf.tpu.experimental.initialize_tpu_system(TPU)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(TPU)\n",
    "    BATCH_SIZE = 128\n",
    "    # drop_remainder must be True if running on TPU, maybe a bug\n",
    "    # so we pad some examples.\n",
    "    nq_test_jsonl_path2 = NQ_TEST_JSONL_PATH + '.pad'\n",
    "    !cp $NQ_TEST_JSONL_PATH $nq_test_jsonl_path2\n",
    "    !tail -n 3 $NQ_TEST_JSONL_PATH >> $nq_test_jsonl_path2\n",
    "    NQ_TEST_JSONL_PATH = nq_test_jsonl_path2\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    BATCH_SIZE = 16\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert-baseline\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModel)                ((None, 1024), (None 335141888   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "logits (TDense)                 (None, 512, 2)       2050        bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 512, 1), (No 0           logits[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "unique_id (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_squeeze (Tens [(None, 512)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_squeeze (Tensor [(None, 512)]        0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "ans_type (TDense)               (None, 5)            5125        bert[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 335,149,063\n",
      "Trainable params: 335,149,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    model = mk_model(config)\n",
    "    model.summary()\n",
    "    cpkt = tf.train.Checkpoint(model=model)\n",
    "    cpkt.restore(CPKT_PATH).assert_consumed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# small_config = config.copy()\n",
    "# small_config['vocab_size']=16\n",
    "# small_config['hidden_size']=64\n",
    "# small_config['max_position_embeddings'] = 32\n",
    "# small_config['num_hidden_layers'] = 4\n",
    "# small_config['num_attention_heads'] = 4\n",
    "# small_config['intermediate_size'] = 256\n",
    "# small_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='', netloc='', path='./nq-test.tfrecords', params='', query='', fragment='')\n",
      "Reading: ../input/tensorflow2-question-answering/simplified-nq-test.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b2eb9227e140629a103ffe2e3b756a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "number of test examples: 9070, written to file: 9070\n"
     ]
    }
   ],
   "source": [
    "if not url_exists(NQ_TEST_TFRECORD_PATH):\n",
    "# if True:\n",
    "    # tf2baseline.FLAGS.max_seq_length = 512\n",
    "    eval_writer = bert_utils.FeatureWriter(filename=NQ_TEST_TFRECORD_PATH,\n",
    "                                           is_training=False)\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_PATH,\n",
    "                                           do_lower_case=True)\n",
    "    features = []\n",
    "    convert = bert_utils.ConvertExamples2Features(tokenizer=tokenizer,\n",
    "                                                  is_training=False,\n",
    "                                                  output_fn=eval_writer.process_feature,\n",
    "                                                  collect_stat=False)\n",
    "    n_examples = 0\n",
    "    # tqdm_notebook = tqdm.tqdm_notebook  # if not on_kaggle_server else None\n",
    "    for examples in bert_utils.nq_examples_iter(input_file=NQ_TEST_JSONL_PATH,\n",
    "                                                is_training=False,\n",
    "                                                tqdm=tqdm):\n",
    "        for example in examples:\n",
    "            n_examples += convert(example)\n",
    "    eval_writer.close()\n",
    "    print('number of test examples: %d, written to file: %d' % (n_examples, eval_writer.num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    567/Unknown - 624s 1s/step"
     ]
    }
   ],
   "source": [
    "raw_ds = tf.data.TFRecordDataset(NQ_TEST_TFRECORD_PATH)\n",
    "decoded_ds = raw_ds.map(_decode_record)\n",
    "batched_ds = decoded_ds.batch(batch_size=BATCH_SIZE, drop_remainder=(TPU is not None))\n",
    "\n",
    "result = model.predict(batched_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add example_id to beginning.\n",
    "example_id_ds = raw_ds.map(lambda x: tf.io.parse_single_example(\n",
    "    serialized=x,\n",
    "    features={\"example_id\": tf.io.FixedLenFeature([], tf.int64)}\n",
    ")['example_id'])\n",
    "result = (np.array(list(example_id_ds)[:len(result[0])]), *result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1- Understanding the code\n",
    "#### For a better understanding, I will briefly explain here.\n",
    "#### In the item \"answer_type\", in the last lines of this block, it is responsible for storing the identified response type, which, according to [github project repository](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py) can be:\n",
    "UNKNOWN = 0\n",
    "YES = 1\n",
    "NO = 2\n",
    "SHORT = 3\n",
    "LONG = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting candidates...\n",
      "Reading examples from: ../input/tensorflow2-question-answering/simplified-nq-test.jsonl\n"
     ]
    }
   ],
   "source": [
    "print(\"getting candidates...\")\n",
    "candidates_dict = read_candidates('../input/tensorflow2-question-answering/simplified-nq-test.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting result_df...\n"
     ]
    }
   ],
   "source": [
    "print(\"getting result_df...\")\n",
    "result_df = pd.DataFrame({\n",
    "    \"example_id\": result[0].squeeze().tolist(),\n",
    "    \"unique_id\": result[1].squeeze().tolist(),\n",
    "    \"start_logits\": result[2].tolist(),\n",
    "    \"end_logits\": result[3].tolist(),\n",
    "    \"answer_type_logits\": result[4].tolist()\n",
    "}).set_index(['example_id', 'unique_id'])\n",
    "# we pad some instances when using TPU, deduplicate it here.\n",
    "if TPU is not None:\n",
    "    print('result_df len before dedup: ' + str(len(result_df)))\n",
    "    result_df = result_df[~result_df.index.duplicated()]\n",
    "    print('result_df len after  dedup: ' + str(len(result_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting token_map_df...\n"
     ]
    }
   ],
   "source": [
    "token_map_ds = raw_ds.map(lambda x: tf.io.parse_single_example(\n",
    "    serialized=x,\n",
    "    features={\n",
    "        \"example_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        \"unique_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        # token_map: token to origin map.\n",
    "        \"token_map\": tf.io.FixedLenFeature([SEQ_LENGTH], tf.int64)\n",
    "    }\n",
    "))\n",
    "print(\"getting token_map_df...\")\n",
    "token_map_df = pd.DataFrame.from_records(list(token_map_ds)).applymap(\n",
    "    lambda x: x.numpy()\n",
    ").set_index(['example_id', 'unique_id'])\n",
    "# we pad some instances when using TPU, deduplicate it here.\n",
    "if TPU is not None:\n",
    "    print('token_map_df len before dedup: ' + str(len(token_map_df)))\n",
    "    token_map_df = token_map_df[~token_map_df.index.duplicated()]\n",
    "    print('token_map_df len after  dedup: ' + str(len(token_map_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "joined = result_df.join(token_map_df, on=['example_id', 'unique_id'])\n",
    "\n",
    "pred_df = pd.DataFrame(columns=['example_id', 'score', 'answer_type',\n",
    "                                'short_span_start', 'short_span_end',\n",
    "                                'long_span_start', 'long_span_end', ]\n",
    "                       ).set_index('example_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932ef986693a43c09c76621724776b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=346), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def best_score_start_end_of_instance(res: pd.Series):\n",
    "    \"\"\"\n",
    "    :param res: index: ['answer_type_logits', 'end_logits', 'start_logits', 'token_map', 'candidates']\n",
    "    :return: best_score_of_instance, start_short_idx, end_short_idx\n",
    "    \"\"\"\n",
    "    msk_invalid_token = np.array(res['token_map']) == -1\n",
    "    s_logits, e_logits = pd.Series(res['start_logits']), pd.Series(res['end_logits'])\n",
    "    # filter logits corresponding to context token and rank top-k.\n",
    "    s_msk_not_top_k = s_logits.mask(msk_invalid_token) \\\n",
    "                          .rank(method='min', ascending=False) > FLAGS.n_best_size\n",
    "    s_indexes = np.ma.masked_array(np.arange(s_logits.size),\n",
    "                                   mask=s_msk_not_top_k | msk_invalid_token)\n",
    "    e_msk_not_top_k = e_logits.mask(msk_invalid_token) \\\n",
    "                          .rank(method='min', ascending=False) > FLAGS.n_best_size\n",
    "    e_indexes = np.ma.masked_array(np.arange(e_logits.size),\n",
    "                                   mask=e_msk_not_top_k | msk_invalid_token)\n",
    "    # s_e_msk has shape: [512, 512], end index should greater than start index, otherwise, mask it.\n",
    "    s_e_msk = e_indexes[np.newaxis, :] <= s_indexes[:, np.newaxis]\n",
    "    # short answer length should litter than max_answer_length, otherwise, mask it.\n",
    "    s_e_msk |= (e_indexes[np.newaxis, :] - s_indexes[:, np.newaxis] >= FLAGS.max_answer_length)\n",
    "    # full mask.\n",
    "    s_e_msk = s_e_msk.filled(True)\n",
    "\n",
    "    if s_e_msk.all():  # if all start-end combinations has been masked.\n",
    "        return np.NAN, np.NAN, np.NAN\n",
    "    else:\n",
    "        # broadcast to shape: [512, 512], and set mask=s_e_msk\n",
    "        s_logits_bc = np.ma.array(\n",
    "            np.broadcast_to(s_logits[:, np.newaxis], shape=[s_logits.size, e_logits.size]),\n",
    "            mask=s_e_msk)\n",
    "        e_logits_bc = np.ma.array(\n",
    "            np.broadcast_to(e_logits[np.newaxis, :], shape=[s_logits.size, e_logits.size]),\n",
    "            mask=s_e_msk)\n",
    "        short_span_score = s_logits_bc + e_logits_bc\n",
    "        cls_token_score = s_logits[0] + e_logits[0]\n",
    "        score = short_span_score - cls_token_score\n",
    "        s_short_idx, e_short_idx = divmod(score.argmax(), e_logits.size)\n",
    "\n",
    "        return score.max(), s_short_idx, e_short_idx\n",
    "\n",
    "\n",
    "for example_id, group_df in tqdm(joined.groupby('example_id')):\n",
    "    # group_df: each row got a unique id(unique_id), all rows have a some example_id.\n",
    "    # columns = ['answer_type_logits', 'end_logits', 'start_logits', 'token_map', 'candidates']\n",
    "    group_df = group_df.copy().reset_index(level='example_id', drop=True)\n",
    "    # get best score/start/end and answer type for every instance within same example.\n",
    "    for u_id, res in group_df.iterrows():\n",
    "        answer_type_logits = pd.Series(res['answer_type_logits'], index=ANSWER_TYPE_ORDER)\n",
    "        group_df.loc[u_id, 'ins_answer_type'] = answer_type_logits.idxmax()\n",
    "        ins_score, ins_start, ins_end = best_score_start_end_of_instance(res)\n",
    "        group_df.loc[u_id, 'ins_score'] = ins_score\n",
    "        group_df.loc[u_id, 'ins_short_span_start'] = res['token_map'][ins_start]\n",
    "        # end span should be exclusive, and np.nan + 1 = np.nan\n",
    "        group_df.loc[u_id, 'ins_short_span_end'] = res['token_map'][ins_end] + 1\n",
    "    # we pick instance result who's best score is best among the instances within same example\n",
    "    answer_type = group_df['ins_answer_type'].value_counts().idxmax()\n",
    "    group_df = group_df[group_df['ins_answer_type'] == answer_type]\n",
    "    best_u_id = group_df['ins_score'].idxmax()\n",
    "    if best_u_id is not np.NAN:  # if all instances got no score\n",
    "        short_span_start, short_span_end = group_df.loc[best_u_id, ['ins_short_span_start', 'ins_short_span_end']]\n",
    "        pred_df.loc[example_id, 'score'] = group_df.loc[best_u_id, 'ins_score']\n",
    "        pred_df.loc[example_id, 'short_span_start'] = short_span_start\n",
    "        pred_df.loc[example_id, 'short_span_end'] = short_span_end\n",
    "        # search for long answer span.\n",
    "        for cand in candidates_dict[str(example_id)]:\n",
    "            if cand['top_level'] and cand['start_token'] <= short_span_start and short_span_end <= cand['end_token']:\n",
    "                pred_df.loc[example_id, 'long_span_start'] = cand['start_token']\n",
    "                pred_df.loc[example_id, 'long_span_end'] = cand['end_token']\n",
    "                break\n",
    "        pred_df.loc[example_id, 'answer_type'] = group_df.loc[best_u_id, 'ins_answer_type']\n",
    "        # break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2- Main Change\n",
    "#### Here is the small, but main change: we created an if to check the predicted response type and thus filter / identify the responses that are passed to the submission file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering the Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_short_pred(pred_row: pd.Series):\n",
    "    # score(best short answer) is np.NAN means: there's no short/long answers.\n",
    "    if pred_row['score'] is np.NAN:\n",
    "        return ''\n",
    "    # answer_type can not be np.NAN if score is not np.NAN.\n",
    "    if pred_row['answer_type'] == 'UNKNOWN':\n",
    "        return ''\n",
    "    if pred_row['answer_type'] in ['YES', 'NO']:\n",
    "        return pred_row['answer_type']\n",
    "    if pred_row['answer_type'] in ['SHORT', 'LONG']:\n",
    "        if pred_row['score'] < 7.5:\n",
    "            return ''\n",
    "        else:\n",
    "            return '%d:%d' % (pred_row['short_span_start'], pred_row['short_span_end'])\n",
    "\n",
    "\n",
    "def get_long_pred(pred_row: pd.Series):\n",
    "    # score(best short answer) is np.NAN means: there's no short/long answers.\n",
    "    if pred_row['score'] is np.NAN:\n",
    "        return ''\n",
    "    # answer_type can not be np.NAN if score is not np.NAN.\n",
    "    if pred_row['answer_type'] == 'UNKNOWN':\n",
    "        return ''\n",
    "    if pred_row['answer_type'] in ['YES', 'NO', 'SHORT', 'LONG']:\n",
    "        if pred_row['score'] < 2.8 or pred_row['long_span_start'] is np.NAN:\n",
    "            return ''\n",
    "        else:\n",
    "            return '%d:%d' % (pred_row['long_span_start'], pred_row['long_span_end'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction_df = pred_df.copy()\n",
    "prediction_df['long_pred'] = pred_df.apply(get_long_pred, axis='columns')\n",
    "prediction_df['short_pred'] = pred_df.apply(get_short_pred, axis='columns')\n",
    "prediction_df.index = prediction_df.index.map(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Generating the Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(SAMPLE_SUBMISSION_PATH).set_index('example_id')\n",
    "\n",
    "for eid, row in prediction_df.iterrows():\n",
    "    sample_submission.loc[eid + '_long', 'PredictionString'] = row['long_pred']\n",
    "    sample_submission.loc[eid + '_short', 'PredictionString'] = row['short_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.reset_index().to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None)\n",
    "prediction_df\n",
    "pd.set_option('display.max_rows',50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Yes\n",
    "Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1651666484583736653_short</th>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-3461207570097431362_short</th>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-8260765274544672220_short</th>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-871487000194429353_short</th>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962215690907729115_short</th>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           PredictionString\n",
       "example_id                                 \n",
       "-1651666484583736653_short              YES\n",
       "-3461207570097431362_short              YES\n",
       "-8260765274544672220_short              YES\n",
       "-871487000194429353_short               YES\n",
       "5962215690907729115_short               YES"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_answers = sample_submission[sample_submission['PredictionString'] == 'YES']\n",
    "yes_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "*No\n",
    "Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>418890410382116795_short</th>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         PredictionString\n",
       "example_id                               \n",
       "418890410382116795_short               NO"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_answers = sample_submission[sample_submission['PredictionString'] == 'NO']\n",
    "no_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Balnk\n",
    "Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1011141123527297803_long</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1011141123527297803_short</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1028916936938579349_long</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1028916936938579349_short</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1055197305756217938_short</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9043499022309600719_short</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9204032098950736962_short</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212083134098244596_short</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930196817123445627_long</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930196817123445627_short</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           PredictionString\n",
       "example_id                                 \n",
       "-1011141123527297803_long                  \n",
       "-1011141123527297803_short                 \n",
       "-1028916936938579349_long                  \n",
       "-1028916936938579349_short                 \n",
       "-1055197305756217938_short                 \n",
       "...                                     ...\n",
       "9043499022309600719_short                  \n",
       "9204032098950736962_short                  \n",
       "9212083134098244596_short                  \n",
       "930196817123445627_long                    \n",
       "930196817123445627_short                   \n",
       "\n",
       "[382 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_answers = sample_submission[sample_submission['PredictionString'] == '']\n",
    "blank_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1074129516932871805_short</th>\n",
       "      <td>3153:3155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1521618734431802363_short</th>\n",
       "      <td>1130:1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1651666484583736653_short</th>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-196591140811642071_short</th>\n",
       "      <td>795:798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-2014171866821038290_short</th>\n",
       "      <td>754:756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8838160678370847582_short</th>\n",
       "      <td>675:679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8877554338842696237_short</th>\n",
       "      <td>369:371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8898704006223012351_short</th>\n",
       "      <td>292:293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934950704129184964_short</th>\n",
       "      <td>515:517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958723574737344087_short</th>\n",
       "      <td>1704:1706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>104 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           PredictionString\n",
       "example_id                                 \n",
       "-1074129516932871805_short        3153:3155\n",
       "-1521618734431802363_short        1130:1131\n",
       "-1651666484583736653_short              YES\n",
       "-196591140811642071_short           795:798\n",
       "-2014171866821038290_short          754:756\n",
       "...                                     ...\n",
       "8838160678370847582_short           675:679\n",
       "8877554338842696237_short           369:371\n",
       "8898704006223012351_short           292:293\n",
       "934950704129184964_short            515:517\n",
       "958723574737344087_short          1704:1706\n",
       "\n",
       "[104 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_answers = sample_submission.iloc[np.array(range(0, len(sample_submission), 2)) + 1]\n",
    "short_answers[short_answers['PredictionString'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>example_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1055197305756217938_long</th>\n",
       "      <td>221:335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1074129516932871805_long</th>\n",
       "      <td>3119:3210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1521618734431802363_long</th>\n",
       "      <td>1118:1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1560939115027122612_long</th>\n",
       "      <td>4457:4607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1636829004676923357_long</th>\n",
       "      <td>2144:2234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8994564890124107395_long</th>\n",
       "      <td>254:350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9204032098950736962_long</th>\n",
       "      <td>508:702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9212083134098244596_long</th>\n",
       "      <td>168:241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934950704129184964_long</th>\n",
       "      <td>496:616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958723574737344087_long</th>\n",
       "      <td>938:2597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>206 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          PredictionString\n",
       "example_id                                \n",
       "-1055197305756217938_long          221:335\n",
       "-1074129516932871805_long        3119:3210\n",
       "-1521618734431802363_long        1118:1250\n",
       "-1560939115027122612_long        4457:4607\n",
       "-1636829004676923357_long        2144:2234\n",
       "...                                    ...\n",
       "8994564890124107395_long           254:350\n",
       "9204032098950736962_long           508:702\n",
       "9212083134098244596_long           168:241\n",
       "934950704129184964_long            496:616\n",
       "958723574737344087_long           938:2597\n",
       "\n",
       "[206 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_answers = sample_submission.iloc[np.array(range(0, len(sample_submission), 2)) + 0]\n",
    "long_answers[long_answers['PredictionString'] != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I am only sharing modifications that I believe may help. I left out Tunning and any significant code changes I made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll be grateful if someone gets a better understanding and can share what really impacts the assessment. No need to share code, just knowledge.\n",
    "### Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "13100f3756f446d1886af01d72ad1f66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "17eb742d8be94216a3699681dfbfc49d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d3e06ef58ec8484189bdb6b4a1568e62",
       "max": 346,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_13100f3756f446d1886af01d72ad1f66",
       "value": 346
      }
     },
     "5912b6345de741c2a9c0509f116c220e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "6ad421fe4bf0440890406dc607c8a9e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c6f1b2ea5ed64dbfb970bfcf1d0c98c2",
       "placeholder": "​",
       "style": "IPY_MODEL_f5b57cbd1a9e4345b3c7faaee3dacb5a",
       "value": " 346/346 [01:51&lt;00:00,  3.10it/s]"
      }
     },
     "7a19198b4bff41ecae8a97cf8aee342f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "932ef986693a43c09c76621724776b8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_17eb742d8be94216a3699681dfbfc49d",
        "IPY_MODEL_6ad421fe4bf0440890406dc607c8a9e3"
       ],
       "layout": "IPY_MODEL_a44072c7a165454e94afdfc12aa9d462"
      }
     },
     "9a0bb8857e84483eb566df1d0171ab20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a19198b4bff41ecae8a97cf8aee342f",
       "placeholder": "​",
       "style": "IPY_MODEL_f78b7491a3fa43e9aa7723a08b5e24ae",
       "value": " 346/? [02:39&lt;00:00,  2.16it/s]"
      }
     },
     "a44072c7a165454e94afdfc12aa9d462": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c57ed1e0894747389b1432b78503a77b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c6f1b2ea5ed64dbfb970bfcf1d0c98c2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d3e06ef58ec8484189bdb6b4a1568e62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8b2eb9227e140629a103ffe2e3b756a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fe6df7a684cc4e01b624c3bf0a1fdfa5",
        "IPY_MODEL_9a0bb8857e84483eb566df1d0171ab20"
       ],
       "layout": "IPY_MODEL_c57ed1e0894747389b1432b78503a77b"
      }
     },
     "dc670be29a68414291a4c78db0afa2c0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5b57cbd1a9e4345b3c7faaee3dacb5a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f78b7491a3fa43e9aa7723a08b5e24ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fe6df7a684cc4e01b624c3bf0a1fdfa5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dc670be29a68414291a4c78db0afa2c0",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5912b6345de741c2a9c0509f116c220e",
       "value": 1
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}